# -*- coding: utf-8 -*-
# ===========================================================
# å»ºç‰©è¨ºæ–­ãã‚“ï¼ˆã‚¹ãƒãƒ›å¯¾å¿œãƒ»ãƒãƒ†ãƒªã‚¢ãƒ«ãƒ‡ã‚¶ã‚¤ãƒ³é¢¨UIãƒ»æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆé©ç”¨ï¼‰
# - å¯è¦–/èµ¤å¤– ç”»åƒï¼šã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ + ã‚«ãƒ¡ãƒ©
# - ç«¯æœ«ã®ç¾åœ¨åœ°ï¼ˆHTML5 Geolocationï¼‰â†’ Foliumåœ°å›³ã«è¡¨ç¤ºï¼ˆæ‰‹å…¥åŠ›ã‚‚å¯ï¼‰
#   * streamlit-geolocation ãŒã‚ã‚Œã°åˆ©ç”¨ï¼ç„¡ã‘ã‚Œã°JSãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
# - RAGï¼šè»½é‡BM25ï¼‹ãƒ–ãƒ¼ã‚¹ãƒˆï¼ˆIR/é–¾å€¤/è‡ªæ²»ä½“ï¼‰â†’ Gemini 2.5 Flash ã§è©³ç´°åˆ†æ
# - çµæœã®ã¿è¡¨ç¤ºï¼šç·åˆè©•ä¾¡ã‚«ãƒ¼ãƒ‰ã‚’å…ˆé ­ã«ã€è©³ç´°ã¯å±•é–‹
# - ãƒ¬ãƒãƒ¼ãƒˆDLï¼ˆå…±æœ‰ï¼‰
# - æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆï¼ˆNoto Sans JP / Noto Serif JPï¼‰
# Python 3.12 äº’æ›
# ===========================================================

# Python 3.12: pkgutil.ImpImporterå‰Šé™¤å›é¿ï¼ˆå¤ã„ä¾å­˜ã®ãŸã‚ã®ä¿é™ºï¼‰
import pkgutil
if not hasattr(pkgutil, "ImpImporter"):
    pkgutil.ImpImporter = pkgutil.zipimporter

import os
import io
import re
import math
import base64
import unicodedata
from typing import List, Tuple, Dict, Optional, Any

import streamlit as st
import requests
import PyPDF2
from PIL import Image

# åœ°å›³
import folium                              # pip install folium
from streamlit_folium import st_folium     # pip install streamlit-folium
from streamlit import components

# --- ã‚ªãƒ—ã‚·ãƒ§ãƒ³: streamlit-geolocation ãŒã‚ã‚Œã°ä½¿ã† ---
HAVE_GEO = False
try:
    from streamlit_geolocation import st_geolocation  # pip install streamlit-geolocation
    HAVE_GEO = True
except Exception:
    HAVE_GEO = False

# -----------------------------------------------------------
# å®šæ•°
# -----------------------------------------------------------
APP_TITLE = "å»ºç‰©è¨ºæ–­ãã‚“"
PDF_SOURCES = [
    ("Structure_Base.pdf", "Structure_Base.pdf"),
    ("ä¸Šå³¶ç”º å…¬å…±æ–½è¨­ç­‰ç·åˆç®¡ç†è¨ˆç”»", "kamijimachou_Public_facility_management_plan.pdf"),
    ("æ¸¯åŒº å…¬å…±æ–½è¨­ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆè¨ˆç”»", "minatoku_Public_facility_management_plan.pdf"),
]

MAX_SNIPPETS = 6         # LLMã¸æ¸¡ã™æŠœç²‹æ•°
MAX_SNIPPET_CHARS = 900  # å„æŠœç²‹ã®æœ€å¤§é•·

# RAGç”¨ã‚·ãƒãƒ‹ãƒ ï¼ˆç°¡æ˜“æ‹¡å¼µï¼‰â€»å¿…è¦ã«å¿œã˜ã¦è¿½åŠ 
QUERY_SYNONYMS = {
    "ã²ã³å‰²ã‚Œ": ["ã²ã³å‰²ã‚Œ", "ã‚¯ãƒ©ãƒƒã‚¯", "äº€è£‚", "ã²ã³"],
    "æµ®ã": ["æµ®ã", "ã†ã"],
    "å‰¥é›¢": ["å‰¥é›¢", "ã¯ãé›¢", "ã¯ãã‚Š"],
    "å«æ°´": ["å«æ°´", "å«æ¹¿", "æµ¸æ°´", "æ¼æ°´", "é›¨æ°´ä¾µå…¥"],
    "èµ¤å¤–ç·š": ["èµ¤å¤–ç·š", "IR", "ã‚µãƒ¼ãƒ¢", "ç†±ç”»åƒ", "ã‚µãƒ¼ãƒ¢ã‚°ãƒ©ãƒ•ã‚£"],
    "åŸºæº–": ["åŸºæº–", "åˆ¤å®š", "è©•ä¾¡", "é–¾å€¤"],
    "ã‚¿ã‚¤ãƒ«": ["ã‚¿ã‚¤ãƒ«", "ç£å™¨è³ªã‚¿ã‚¤ãƒ«", "ãƒ¢ã‚¶ã‚¤ã‚¯ã‚¿ã‚¤ãƒ«"],
    "ALC": ["ALC", "è»½é‡æ°—æ³¡ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆ"],
    "ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆ": ["ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆ", "RC", "ä¸­æ€§åŒ–"],
    "é˜²æ°´": ["é˜²æ°´", "å¡—è†œ", "ã‚·ãƒ¼ãƒªãƒ³ã‚°", "ã‚·ãƒ¼ãƒ©ãƒ³ãƒˆ"],
    "åŠ£åŒ–åº¦": ["åŠ£åŒ–åº¦", "ã‚°ãƒ¬ãƒ¼ãƒ‰", "åŒºåˆ†", "A", "B", "C", "D"],
}

# -----------------------------------------------------------
# æ–‡å­—åˆ—ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# -----------------------------------------------------------
def normalize_text(text: str) -> str:
    text = unicodedata.normalize("NFKC", text)
    text = text.replace("\r", " ").replace("\n", " ")
    text = re.sub(r"\s+", " ", text).strip()
    return text

def tokenize(s: str) -> List[str]:
    # å½¢æ…‹ç´ è§£æãªã—ã®è»½é‡ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆæ•°å­—ãƒ»è¨˜å·ã‚’æ‰±ã„ã¤ã¤æ—¥æœ¬èªå¯¾å¿œï¼‰
    s = s.lower()
    s = re.sub(r"[^\wä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³ï¼…â„ƒï¼\.]", " ", s)
    s = s.replace("ï¼", ".")
    s = re.sub(r"\s+", " ", s).strip()
    return s.split()

def query_expand_tokens(q: str) -> List[str]:
    tokens = set(tokenize(q))
    for key, syns in QUERY_SYNONYMS.items():
        if key in q or any(s in q for s in syns):
            for s in syns:
                for t in tokenize(s):
                    tokens.add(t)
    return list(tokens)

# -----------------------------------------------------------
# PDF â†’ ãƒãƒ£ãƒ³ã‚¯ï¼ˆãƒšãƒ¼ã‚¸ç•ªå·ä»˜ãï¼‰
# -----------------------------------------------------------
def extract_chunks_from_pdf(pdf_path: str, title: str,
                            max_chars: int = 900, overlap: int = 120) -> List[Dict[str, Any]]:
    """å„PDFã‚’ãƒšãƒ¼ã‚¸å˜ä½ã§èª­ã¿ã€ãã®ä¸Šã§æ–‡å­—é•·ãƒãƒ£ãƒ³ã‚¯åŒ–ã€‚ãƒšãƒ¼ã‚¸ç¯„å›²ã‚’ä¿æŒã€‚"""
    if not os.path.exists(pdf_path):
        return []
    chunks: List[Dict[str, Any]] = []
    try:
        with open(pdf_path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            buf = ""
            page_start = 1
            for i, page in enumerate(reader.pages, start=1):
                t = page.extract_text() or ""
                # ãƒšãƒ¼ã‚¸åŒºåˆ‡ã‚Šã¯è»½ãä¿æŒ
                page_text = normalize_text(t)
                if not page_text:
                    continue
                # ãƒšãƒ¼ã‚¸ãƒ†ã‚­ã‚¹ãƒˆã‚’max_charsã§åˆ†å‰²
                pos = 0
                while pos < len(page_text):
                    end = min(pos + max_chars, len(page_text))
                    ch = page_text[pos:end].strip()
                    if ch:
                        chunks.append({
                            "doc": title,
                            "path": pdf_path,
                            "text": ch,
                            "page_start": i,
                            "page_end": i,  # 1ãƒšãƒ¼ã‚¸å†…ã§åˆ‡ã£ã¦ã„ã‚‹ã®ã§ç­‰ã—ã„
                        })
                    # ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
                    pos = end - overlap if (end - overlap) > pos else end
    except Exception as e:
        chunks.append({
            "doc": title, "path": pdf_path,
            "text": f"[PDFèª­è¾¼ã‚¨ãƒ©ãƒ¼:{pdf_path}:{e}]",
            "page_start": None, "page_end": None
        })
    return chunks

# -----------------------------------------------------------
# RAGã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼šBM25ï¼ˆè»½é‡ï¼‰ï¼‹ãƒ¡ã‚¿ãƒ–ãƒ¼ã‚¹ãƒˆ
# -----------------------------------------------------------
class BM25Index:
    def __init__(self, k1: float = 1.6, b: float = 0.75):
        self.k1 = k1
        self.b = b
        self.N = 0
        self.avgdl = 0.0
        self.df: Dict[str, int] = {}
        self.doc_len: List[int] = []
        self.postings: Dict[str, List[Tuple[int, int]]] = {}  # term -> [(doc_id, tf), ...]
        self.docs: List[Dict[str, Any]] = []  # ãƒ¡ã‚¿å«ã‚€

    @staticmethod
    def _tf(tokens: List[str]) -> Dict[str, int]:
        tf: Dict[str, int] = {}
        for t in tokens:
            tf[t] = tf.get(t, 0) + 1
        return tf

    def build(self, docs: List[Dict[str, Any]]):
        """docs: {"text", "doc", "page_start", "page_end", ...}"""
        self.docs = docs
        self.N = len(docs)
        lengths = []
        # äº‹å‰ã«å…¨DF/ãƒã‚¹ãƒ†ã‚£ãƒ³ã‚°ä½œæˆ
        for doc_id, d in enumerate(docs):
            tokens = tokenize(d["text"])
            d["_tokens"] = tokens
            tf = self._tf(tokens)
            lengths.append(len(tokens))
            for term, c in tf.items():
                # df
                self.df[term] = self.df.get(term, 0) + 1
                # postings
                self.postings.setdefault(term, []).append((doc_id, c))
        self.doc_len = lengths
        self.avgdl = (sum(lengths) / self.N) if self.N else 0.0

    def idf(self, term: str) -> float:
        df = self.df.get(term, 0)
        # Okapi BM25ã®IDFï¼ˆ+1å®‰å®šé …ï¼‰
        return math.log((self.N - df + 0.5) / (df + 0.5) + 1.0)

    def score_doc(self, q_tokens: List[str], doc_id: int) -> float:
        score = 0.0
        dl = self.doc_len[doc_id] if doc_id < len(self.doc_len) else 0
        if dl == 0:
            return 0.0
        # ã‚¯ã‚¨ãƒªå†…é‡è¤‡ã¯ç„¡è¦–ï¼ˆé›†åˆã§OKï¼‰
        for term in set(q_tokens):
            plist = self.postings.get(term)
            if not plist:
                continue
            # doc_idã®tfã‚’æ¢ã™
            tf = 0
            # postingsã¯é€šå¸¸docã”ã¨ã«1ä»¶
            # ï¼ˆä½ãƒ¡ãƒ¢ãƒªã®ãŸã‚ç·šå½¢æ¢ç´¢ã ãŒã€ã‚³ãƒ¼ãƒ‘ã‚¹å°è¦æ¨¡ã‚’æƒ³å®šï¼‰
            for did, c in plist:
                if did == doc_id:
                    tf = c
                    break
            if tf == 0:
                continue
            idf = self.idf(term)
            denom = tf + self.k1 * (1 - self.b + self.b * (dl / (self.avgdl or 1.0)))
            score += idf * (tf * (self.k1 + 1)) / (denom or 1.0)
        return score

# -----------------------------------------------------------
# RAGæ§‹ç¯‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
# -----------------------------------------------------------
@st.cache_resource(show_spinner=False)
def build_rag() -> Dict[str, Any]:
    # 1) PDF â†’ ãƒ¡ã‚¿ä»˜ããƒãƒ£ãƒ³ã‚¯
    all_chunks: List[Dict[str, Any]] = []
    for title, path in PDF_SOURCES:
        all_chunks.extend(extract_chunks_from_pdf(path, title))
    # 2) BM25ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
    bm25 = BM25Index()
    if all_chunks:
        bm25.build(all_chunks)
    # 3) è¿½åŠ ãƒ¡ã‚¿ï¼šæ•°å€¤é–¾å€¤ãƒ»IRé–¢é€£èªã®æœ‰ç„¡
    for d in all_chunks:
        txt = d["text"]
        d["_has_numbers"] = bool(re.search(r"\b\d+(\.\d+)?\s*(mm|ãœ|ï¼…|%|â„ƒ)\b", txt))
        d["_has_ir"] = any(k in txt for k in ["èµ¤å¤–ç·š", "ã‚µãƒ¼ãƒ¢", "IR", "ç†±ç”»åƒ", "æ”¾å°„ç‡", "åå°„æ¸©åº¦"])
    return {"index": bm25, "docs": all_chunks}

# -----------------------------------------------------------
# RAGæ¤œç´¢ï¼ˆBM25 + ãƒ–ãƒ¼ã‚¹ãƒˆï¼‰
# -----------------------------------------------------------
def rag_search(query: str, have_ir: bool, k: int = MAX_SNIPPETS) -> List[Dict[str, Any]]:
    data = build_rag()
    bm25: BM25Index = data["index"]
    docs = data["docs"]
    if not docs:
        return []

    q_tokens = query_expand_tokens(query)

    # ãƒ–ãƒ¼ã‚¹ãƒˆãƒ•ãƒ©ã‚°
    want_threshold = any(w in query for w in ["åŸºæº–", "é–¾å€¤", "å¹…", "mm", "ï¼…", "%", "â„ƒ", "æ¸©åº¦"])
    boost_muni = None
    if "æ¸¯åŒº" in query:
        boost_muni = "æ¸¯åŒº"
    elif "ä¸Šå³¶ç”º" in query or "ä¸Šå¶‹ç”º" in query:
        boost_muni = "ä¸Šå³¶ç”º"

    scored: List[Tuple[float, int]] = []
    for doc_id, d in enumerate(docs):
        base = bm25.score_doc(q_tokens, doc_id)

        # é–¾å€¤ç³»ã‚’æ±‚ã‚ã¦ã„ã‚‹ â†’ æ•°å€¤å…¥ã‚ŠæŠœç²‹ã‚’å¾®ãƒ–ãƒ¼ã‚¹ãƒˆ
        if want_threshold and d.get("_has_numbers"):
            base *= 1.12

        # IRç”»åƒãŒã‚ã‚‹ â†’ IRé–¢é€£ã®ç¯€ã‚’å¾®ãƒ–ãƒ¼ã‚¹ãƒˆ
        if have_ir and d.get("_has_ir"):
            base *= 1.10

        # è‡ªæ²»ä½“ãƒ–ãƒ¼ã‚¹ãƒˆï¼ˆè³ªå•ã«è‡ªæ²»ä½“ãŒå«ã¾ã‚Œã‚‹ã¨ãï¼‰
        if boost_muni:
            if boost_muni in d["doc"]:
                base *= 1.15

        if base > 0:
            scored.append((base, doc_id))

    scored.sort(key=lambda x: x[0], reverse=True)
    top = [docs[doc_id] for (_, doc_id) in scored[: k]]
    # é•·ã™ãã‚‹æŠœç²‹ã¯ã‚«ãƒƒãƒˆ
    for t in top:
        if len(t["text"]) > MAX_SNIPPET_CHARS:
            t["text"] = t["text"][:MAX_SNIPPET_CHARS] + "â€¦"
    return top

# -----------------------------------------------------------
# ç”»åƒ â†’ Geminiã‚¤ãƒ³ãƒ©ã‚¤ãƒ³
# -----------------------------------------------------------
def image_to_inline_part(image: Image.Image, max_width: int = 1400) -> Dict:
    if image.mode != "RGB":
        image = image.convert("RGB")
    if image.width > max_width:
        r = max_width / float(image.width)
        image = image.resize((max_width, int(image.height * r)))
    buf = io.BytesIO()
    image.save(buf, format="JPEG", quality=90, optimize=True)
    b64 = base64.b64encode(buf.getvalue()).decode("utf-8")
    return {"inline_data": {"mime_type": "image/jpeg", "data": b64}}

# -----------------------------------------------------------
# Gemini å‘¼ã³å‡ºã—ï¼ˆmodels/gemini-2.5-flashï¼‰
# -----------------------------------------------------------
def call_gemini(api_key: str, prompt_text: str, image_parts: List[Dict]) -> Dict:
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={api_key}"
    headers = {"Content-Type": "application/json"}
    parts = [{"text": prompt_text}]
    parts.extend(image_parts)
    payload = {"contents": [{"parts": parts}]}
    resp = requests.post(url, headers=headers, json=payload, timeout=120)
    resp.raise_for_status()
    return resp.json()

def extract_text_from_gemini(result: Dict) -> str:
    try:
        return result["candidates"][0]["content"]["parts"][0]["text"]
    except Exception:
        return ""

# -----------------------------------------------------------
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆâ€œçµæœã®ã¿â€ï¼‹æ ¹æ‹ ãƒšãƒ¼ã‚¸ã‚’å¿…é ˆè¡¨ç¤ºï¼‰
# -----------------------------------------------------------
def build_master_prompt(user_q: str,
                        rag_snippets: List[Dict[str, Any]],
                        ir_meta: Dict) -> str:
    # RAGæŠœç²‹ï¼ˆå‡ºå…¸ï¼‹ãƒšãƒ¼ã‚¸ï¼‰
    rag_lines = []
    for d in rag_snippets:
        pg = ""
        if d.get("page_start") is not None:
            if d["page_end"] and d["page_end"] != d["page_start"]:
                pg = f" p.{d['page_start']}-{d['page_end']}"
            else:
                pg = f" p.{d['page_start']}"
        rag_lines.append(f"[{d['doc']}{pg}] {d['text']}")
    rag_text = "\n".join(rag_lines)

    ir_note = ""
    if ir_meta.get("has_ir"):
        ir_note = (
            "æ³¨: æ·»ä»˜ã«ã¯èµ¤å¤–ç·šç”»åƒãŒå«ã¾ã‚Œã¾ã™ã€‚æ¸©åº¦åˆ†å¸ƒãƒ»ç†±ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ»æ½œåœ¨çš„å«æ°´ã‚„æ–­ç†±ä¸è‰¯ç­‰ã®å¯èƒ½æ€§ã«ç•™æ„ã—ã€"
            "IRãƒ¡ã‚¿ï¼ˆÎµ/åå°„æ¸©åº¦/å¤–æ°—æ¸©/æ¹¿åº¦/è·é›¢/è§’åº¦/ãƒ¬ãƒ³ã‚¸ï¼‰ä¸è¶³æ™‚ã¯ä¿¡é ¼åº¦ã‚’ä¸‹ã’ã¦ã‚³ãƒ¡ãƒ³ãƒˆã—ã¦ãã ã•ã„ã€‚\n"
            f"IRãƒ¡ã‚¿: Îµ={ir_meta.get('emissivity','ä¸æ˜')}, T_ref={ir_meta.get('t_ref','ä¸æ˜')}â„ƒ, "
            f"T_amb={ir_meta.get('t_amb','ä¸æ˜')}â„ƒ, RH={ir_meta.get('rh','ä¸æ˜')}%, "
            f"è·é›¢={ir_meta.get('dist','ä¸æ˜')}m, è§’åº¦={ir_meta.get('angle','ä¸æ˜')}Â°.\n"
        )

    prompt = f"""
ã‚ãªãŸã¯éç ´å£Šæ¤œæŸ»ãƒ»å»ºç¯‰ãƒ»ææ–™å­¦ã®ä¸Šç´šè¨ºæ–­å£«ã€‚å›½åœŸäº¤é€šçœï¼ˆMLITï¼‰ãŠã‚ˆã³è‡ªæ²»ä½“åŸå…¸PDFã®æ ¹æ‹ ã®ã¿ã‚’ç”¨ã„ã¦ã€æ—¥æœ¬èªã§ç°¡æ½”ã«â€œçµæœã®ã¿â€ã‚’ä½œæˆã›ã‚ˆã€‚
ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã¯ç¦æ­¢ã€‚æ•°å€¤é–¾å€¤ã‚„å®šç¾©ã¯RAGæŠœç²‹ã«ã‚ã‚‹å ´åˆã®ã¿ä½¿ç”¨ã€‚ç„¡ã„å ´åˆã¯ã€Œæœªæ²è¼‰ã€ã¨æ˜ç¤ºã€‚
å„ä¸»å¼µã« **[å‡ºå…¸: æ–‡æ›¸å Â§/è¦‹å‡ºã— p.xx]** ã‚’ä½µè¨˜ã™ã‚‹ã“ã¨ï¼ˆæœ¬æ–‡ä¸­ã§OKï¼‰ã€‚

# å…¥åŠ›
- ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•: {user_q}
- RAGæŠœç²‹ï¼ˆå‡ºæ‰€ä»˜ãã€ã“ã‚Œä»¥å¤–ã¯æ ¹æ‹ ã«ã—ãªã„ï¼‰:
{rag_text}

{ir_note}
# å‡ºåŠ›ä»•æ§˜ï¼ˆMarkdownã€â€œçµæœã®ã¿â€ï¼‰
1. **ç·åˆè©•ä¾¡**
   - ã‚°ãƒ¬ãƒ¼ãƒ‰ï¼ˆA/B/C/Dï¼‰ã¨ä¸»å› ï¼ˆ1â€“2è¡Œï¼‰
   - æ¨å®šæ®‹å­˜å¯¿å‘½ï¼ˆå¹…ã€ä¾‹ï¼š7â€“12å¹´ï¼‰ï¼ï¼ˆä»»æ„ï¼‰è£œä¿®å¾Œã®æœŸå¾…å¯¿å‘½
2. **ä¸»è¦æ ¹æ‹ ï¼ˆå¯è¦–/IR/NDTï¼‰** # ç”»åƒã‹ã‚‰èª­ã¿å–ã‚Œã‚‹äº‹å®Ÿã«é™å®šã€æ ¹æ‹ å‡ºå…¸ã‚’ä½µè¨˜
3. **MLIT/è‡ªæ²»ä½“åŸºæº–ã¨ã®é–¢ä¿‚** # æ–‡æ›¸åï¼‹ç¯€/è¦‹å‡ºã—ï¼‹ãƒšãƒ¼ã‚¸
4. **æ¨å¥¨å¯¾å¿œï¼ˆå„ªå…ˆåº¦é †ï¼‰**
5. **é™ç•Œã¨è¿½åŠ ãƒ‡ãƒ¼ã‚¿è¦æœ›**

æ³¨æ„: ç”»åƒã‹ã‚‰ç›´æ¥æ¸¬ã‚Œãªã„å€¤ï¼ˆã²ã³å¹…ç­‰ï¼‰ã¯æ–­å®šã—ãªã„ã€‚IRãƒ¡ã‚¿ä¸è¶³æ™‚ã¯ä¿¡é ¼åº¦ã‚’ä¸‹ã’ã‚‹æ—¨ã‚’æ˜è¨˜ã€‚
""".strip()
    return normalize_text(prompt)

# -----------------------------------------------------------
# UI: ãƒãƒ†ãƒªã‚¢ãƒ«CSS/ãƒ•ã‚©ãƒ³ãƒˆã¯ components.html ã§éè¡¨ç¤ºæŒ¿å…¥ï¼ˆå…ˆé ­ã«å‡ºãªã„ï¼‰
# -----------------------------------------------------------
def inject_material_css():
    components.v1.html(
        """
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@400;500;700;900&family=Noto+Serif+JP:wght@500;700&display=swap" rel="stylesheet">
<style id="app-style">
:root{
  --mdc-primary:#2962ff; --mdc-primary-variant:#0039cb; --mdc-secondary:#00b8d4;
  --mdc-bg:#f7f9fc; --mdc-surface:#ffffff; --mdc-on-primary:#ffffff;
  --mdc-outline:rgba(0,0,0,.08); --radius:16px; --shadow:0 6px 18px rgba(0,0,0,.08);
  --tap-min:44px;
}
@media (prefers-color-scheme: dark){
  :root{ --mdc-bg:#0f1115; --mdc-surface:#171a21; --mdc-on-primary:#ffffff; --mdc-outline:rgba(255,255,255,.08); }
}
</style>
<style id="app-style-2">
.block-container{padding-top:2.6rem !important;padding-bottom:2rem;overflow:visible !important;}
body{background:var(--mdc-bg);}
.jp-sans{font-family:'Noto Sans JP',-apple-system,BlinkMacSystemFont,'Segoe UI','Hiragino Kaku Gothic ProN','Hiragino Sans','Meiryo',sans-serif!important;line-height:1.7;letter-spacing:.01em;}
.jp-serif{font-family:'Noto Serif JP','Hiragino Mincho ProN','Yu Mincho',serif!important;line-height:1.8;letter-spacing:.01em;}
.jp-report h1,.jp-report h2,.jp-report h3,.jp-report p,.jp-report li,.jp-report table{font-family:'Noto Sans JP',-apple-system,BlinkMacSystemFont,'Segoe UI','Hiragino Kaku Gothic ProN','Hiragino Sans','Meiryo',sans-serif!important;line-height:1.6;}
.app-hero{background:linear-gradient(135deg,var(--mdc-primary),var(--mdc-secondary));color:#fff;border-radius:20px;box-shadow:var(--shadow);padding:14px 16px;margin:0 0 14px 0;position:relative;overflow:visible;}
.app-hero-title{font-family:'Noto Sans JP',sans-serif;font-weight:900;font-size:1.45rem;line-height:1.25;margin:0 0 4px 0;text-shadow:0 1px 2px rgba(0,0,0,.18);word-break:keep-all;overflow-wrap:anywhere;}
.app-hero-sub{font-family:'Noto Sans JP',sans-serif;font-weight:500;font-size:.95rem;line-height:1.5;opacity:.95;margin:0;}
.md-card{background:var(--mdc-surface);border-radius:var(--radius);box-shadow:var(--shadow);padding:1rem 1.1rem;margin:0 0 1rem 0;border:1px solid var(--mdc-outline);}
.md-title{font-size:1.1rem;font-weight:700;margin:0 0 .6rem 0;}
.md-sub{opacity:.85;font-size:.95rem;}
.eval-chip{display:inline-block;padding:.35rem .7rem;border-radius:999px;color:#fff;font-weight:700;letter-spacing:.02em;background:linear-gradient(135deg,var(--mdc-primary),var(--mdc-secondary));}
.good-shadow{box-shadow:var(--shadow);}
.stButton>button,.stTextInput input,.stFileUploader label,.stCameraInput label,.stNumberInput input{min-height:var(--tap-min);border-radius:12px!important;font-weight:600;}
.stButton>button{background:linear-gradient(135deg,var(--mdc-primary),var(--mdc-secondary))!important;color:#fff!important;border:none!important;box-shadow:var(--shadow);}
:where(button,input,select,textarea):focus-visible{outline:3px solid color-mix(in srgb,var(--mdc-primary) 60%, white);outline-offset:2px;border-radius:12px;}
</style>
        """,
        height=0,
    )

# -----------------------------------------------------------
# ç«¯æœ«ä½ç½®ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å–å¾—ï¼ˆquery paramsçµŒç”±ï¼‰
# -----------------------------------------------------------
def geolocate_fallback_via_query_params(show_widget: bool = True) -> Tuple[Optional[float], Optional[float]]:
    params = st.experimental_get_query_params()
    lat = params.get("lat", [None])[0]
    lon = params.get("lon", [None])[0]
    if lat and lon:
        try:
            return float(lat), float(lon)
        except Exception:
            return None, None

    if show_widget:
        components.v1.html(
            """
<script>
(function(){
  if (!navigator.geolocation){ document.body.innerText='GEO_UNSUPPORTED'; return; }
  navigator.geolocation.getCurrentPosition(function(pos){
    const lat = pos.coords.latitude.toFixed(6);
    const lon = pos.coords.longitude.toFixed(6);
    const url = new URL(window.location.href);
    url.searchParams.set('lat', lat);
    url.searchParams.set('lon', lon);
    const a = document.createElement('a');
    a.href = url.toString();
    a.target = '_top';
    a.rel = 'opener';
    document.body.appendChild(a);
    a.click();
  }, function(err){
    document.body.innerText = 'GEO_ERROR:' + (err && err.message ? err.message : 'DENIED');
  }, { enableHighAccuracy:true, timeout:10000, maximumAge:0 });
})();
</script>
            """,
            height=0,
        )
    return None, None

# -----------------------------------------------------------
# ãƒ¡ã‚¤ãƒ³
# -----------------------------------------------------------
def main():
    st.set_page_config(page_title=APP_TITLE, layout="wide")
    inject_material_css()

    # ãƒ’ãƒ¼ãƒ­ãƒ¼ãƒ˜ãƒƒãƒ€ãƒ¼
    st.markdown(
        f"""
<div class="app-hero jp-sans">
  <div class="app-hero-title">ğŸ—ï¸ {APP_TITLE}</div>
  <div class="app-hero-sub">ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³æœ€é©åŒ– / ç”»åƒï¼ˆå¯è¦–ãƒ»èµ¤å¤–ï¼‰ï¼‹RAGï¼ˆBM25/ãƒ–ãƒ¼ã‚¹ãƒˆï¼‰ï¼‹Gemini 2.5 Flash</div>
</div>
        """,
        unsafe_allow_html=True
    )

    # --- 1) è³ªå• ---
    st.markdown('<div class="md-card">', unsafe_allow_html=True)
    st.markdown('<div class="md-title">1) è³ªå•</div>', unsafe_allow_html=True)
    user_q = st.text_input("ä¾‹ï¼šå¤–å£ã‚¿ã‚¤ãƒ«ã®ã²ã³å‰²ã‚ŒåŸºæº–ã¨æ¨å®šå¯¿å‘½", "", placeholder="åˆ†æã—ãŸã„ãƒ†ãƒ¼ãƒãƒ»è³ªå•ã‚’å…¥åŠ›")
    st.markdown('</div>', unsafe_allow_html=True)

    # --- 2) ç”»åƒå…¥åŠ›ï¼ˆå¯è¦–/èµ¤å¤–ï¼‰ ---
    st.markdown('<div class="md-card">', unsafe_allow_html=True)
    st.markdown('<div class="md-title">2) ç”»åƒå…¥åŠ›ï¼ˆå¯è¦–/èµ¤å¤–ï¼‰</div>', unsafe_allow_html=True)
    colA, colB = st.columns(2)
    with colA:
        st.markdown("**å¯è¦–ç”»åƒï¼ˆã©ã¡ã‚‰ã‹1ã¤ï¼‰**")
        vis_file = st.file_uploader("å¯è¦–ç”»åƒã‚’é¸æŠï¼ˆJPG/PNGï¼‰", type=["jpg", "jpeg", "png"], key="vis_up")
        vis_cam = st.camera_input("ã‚«ãƒ¡ãƒ©ã§æ’®å½±ï¼ˆå¯è¦–ï¼‰", key="vis_cam")
    with colB:
        st.markdown("**èµ¤å¤–ç·šï¼ˆIRï¼‰ç”»åƒï¼ˆä»»æ„ï¼šå¤–ä»˜ã‘ã‚«ãƒ¡ãƒ©å†™çœŸï¼‰**")
        ir_file = st.file_uploader("IRç”»åƒã‚’é¸æŠï¼ˆJPG/PNGï¼‰", type=["jpg", "jpeg", "png"], key="ir_up")

        with st.expander("IRãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆä»»æ„ãƒ»ç²¾åº¦å‘ä¸Šï¼‰"):
            ir_emiss = st.text_input("æ”¾å°„ç‡ Îµï¼ˆä¾‹: 0.95ï¼‰", "")
            ir_tref  = st.text_input("åå°„æ¸©åº¦ T_ref [â„ƒ]ï¼ˆä¾‹: 20ï¼‰", "")
            ir_tamb  = st.text_input("å¤–æ°—æ¸© T_amb [â„ƒ]ï¼ˆä¾‹: 22ï¼‰", "")
            ir_rh    = st.text_input("ç›¸å¯¾æ¹¿åº¦ RH [%]ï¼ˆä¾‹: 65ï¼‰", "")
            ir_dist  = st.text_input("æ’®å½±è·é›¢ [m]ï¼ˆä¾‹: 5ï¼‰", "")
            ir_ang   = st.text_input("æ’®å½±è§’åº¦ [Â°]ï¼ˆä¾‹: 10ï¼‰", "")
    st.markdown('</div>', unsafe_allow_html=True)

    # ç”»åƒãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
    vis_img = None
    if vis_cam is not None:
        vis_img = Image.open(vis_cam)
    elif vis_file is not None:
        vis_img = Image.open(vis_file)

    ir_img = None
    if ir_file is not None:
        ir_img = Image.open(ir_file)

    # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆã‚«ãƒ¼ãƒ‰ï¼‰
    st.markdown('<div class="md-card">', unsafe_allow_html=True)
    pv_col1, pv_col2 = st.columns(2)
    with pv_col1:
        if vis_img is not None:
            st.image(vis_img, caption="å¯è¦–ç”»åƒ", use_container_width=True)
    with pv_col2:
        if ir_img is not None:
            st.image(ir_img, caption="IRç”»åƒ", use_container_width=True)
    st.markdown('</div>', unsafe_allow_html=True)

    # --- 3) ä½ç½®æƒ…å ±ï¼ˆç«¯æœ«ã®ç¾åœ¨åœ°ï¼‰ ---
    st.markdown('<div class="md-card">', unsafe_allow_html=True)
    st.markdown('<div class="md-title">3) ä½ç½®æƒ…å ±ï¼ˆç¾åœ¨åœ° or æ‰‹å…¥åŠ›ï¼‰</div>', unsafe_allow_html=True)

    lat_val: Optional[float] = None
    lon_val: Optional[float] = None

    if HAVE_GEO:
        loc = st_geolocation(key="geoloc", label="ğŸ“ ç¾åœ¨åœ°ã‚’å–å¾—ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ã®ä½ç½®æƒ…å ±ã‚’è¨±å¯ã—ã¦ãã ã•ã„ï¼‰")
        if isinstance(loc, dict):
            lat_val = loc.get("latitude") or loc.get("lat")
            lon_val = loc.get("longitude") or loc.get("lon")
            try:
                if lat_val is not None: lat_val = float(lat_val)
                if lon_val is not None: lon_val = float(lon_val)
            except Exception:
                lat_val, lon_val = None, None
    else:
        st.info("ç¾åœ¨åœ°å–å¾—ï¼šæ¨™æº–ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®ãŸã‚ç°¡æ˜“æ–¹å¼ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ä¸‹ã®ãƒœã‚¿ãƒ³æŠ¼ä¸‹å¾Œã€ãƒ–ãƒ©ã‚¦ã‚¶è¨±å¯â†’è‡ªå‹•ã§ä½ç½®ãŒå…¥åŠ›ã•ã‚Œã¾ã™ã€‚")
        if st.button("ğŸ“ ç¾åœ¨åœ°ã‚’å–å¾—ï¼ˆç°¡æ˜“æ–¹å¼ï¼‰"):
            geolocate_fallback_via_query_params(show_widget=True)
        lat_val, lon_val = geolocate_fallback_via_query_params(show_widget=False)

    c1, c2, c3 = st.columns([1,1,2])
    with c1:
        lat_str = "" if lat_val is None else f"{lat_val:.6f}"
        lat_str = st.text_input("ç·¯åº¦ï¼ˆä¾‹ï¼š35.6804ï¼‰", value=lat_str, key="lat_manual")
    with c2:
        lon_str = "" if lon_val is None else f"{lon_val:.6f}"
        lon_str = st.text_input("çµŒåº¦ï¼ˆä¾‹ï¼š139.7690ï¼‰", value=lon_str, key="lon_manual")
    with c3:
        st.caption("â€» ç«¯æœ«ã®ä½ç½®æƒ…å ±ãŒå–å¾—ã§ããªã„å ´åˆã¯ã€ç·¯åº¦çµŒåº¦ã‚’æ‰‹å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    # åœ°å›³
    lat_f, lon_f = None, None
    try:
        lat_f = float(lat_str) if lat_str else None
        lon_f = float(lon_str) if lon_str else None
    except Exception:
        lat_f, lon_f = None, None

    if lat_f is not None and lon_f is not None:
        m = folium.Map(location=[lat_f, lon_f], zoom_start=18, tiles="OpenStreetMap")
        folium.Marker([lat_f, lon_f], tooltip="å¯¾è±¡åœ°ç‚¹").add_to(m)
        st_folium(m, height=300, use_container_width=True)
    else:
        st.info("åœ°å›³è¡¨ç¤ºï¼šç·¯åº¦çµŒåº¦ãŒæœªæŒ‡å®šã§ã™ã€‚ä½ç½®æƒ…å ±ã®è¨±å¯ã‚’ä¸ãˆã‚‹ã‹ã€æ‰‹å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
    st.markdown('</div>', unsafe_allow_html=True)

    # --- 4) å®Ÿè¡Œ ---
    st.markdown('<div class="md-card">', unsafe_allow_html=True)
    st.markdown('<div class="md-title">4) è§£æã®å®Ÿè¡Œ</div>', unsafe_allow_html=True)
    run = st.button("ğŸ” Geminiã§è©³ç´°åˆ†æï¼ˆçµæœã®ã¿è¡¨ç¤ºï¼‰", use_container_width=True)
    st.markdown('</div>', unsafe_allow_html=True)

    # -------------------------------------------------------
    # è§£æãƒ•ãƒ­ãƒ¼
    # -------------------------------------------------------
    if run:
        if not user_q:
            st.error("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            return

        # RAGæ¤œç´¢ï¼ˆBM25 + ãƒ–ãƒ¼ã‚¹ãƒˆï¼‰
        have_ir = ir_img is not None
        snippets = rag_search(user_q, have_ir=have_ir, k=MAX_SNIPPETS)

        # ç”»åƒ â†’ Gemini parts
        image_parts = []
        if vis_img is not None:
            image_parts.append(image_to_inline_part(vis_img))
        if ir_img is not None:
            image_parts.append(image_to_inline_part(ir_img))

        # IRãƒ¡ã‚¿ï¼ˆä»»æ„ï¼‰
        ir_meta = {
            "has_ir": have_ir,
            "emissivity": (locals().get('ir_emiss') or "ä¸æ˜").strip(),
            "t_ref": (locals().get('ir_tref') or "ä¸æ˜").strip(),
            "t_amb": (locals().get('ir_tamb') or "ä¸æ˜").strip(),
            "rh": (locals().get('ir_rh') or "ä¸æ˜").strip(),
            "dist": (locals().get('ir_dist') or "ä¸æ˜").strip(),
            "angle": (locals().get('ir_ang') or "ä¸æ˜").strip(),
        }

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        prompt = build_master_prompt(user_q, snippets, ir_meta)

        # Gemini API Key
        try:
            api_key = st.secrets["gemini"]["API_KEY"]
        except (KeyError, FileNotFoundError):
            st.error("Gemini API Key ãŒæœªè¨­å®šã§ã™ã€‚.streamlit/secrets.toml ã« [gemini].API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
            return

        with st.spinner("GeminiãŒåˆ†æä¸­â€¦"):
            try:
                result = call_gemini(api_key, prompt, image_parts)
                report_md = extract_text_from_gemini(result)
            except requests.HTTPError as e:
                st.error(f"APIã‚¨ãƒ©ãƒ¼: {e.response.status_code} {e.response.reason}\n{e.response.text[:500]}")
                return
            except Exception as e:
                st.error(f"å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
                return

        if not report_md:
            st.warning("ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ãŒç©ºã§ã—ãŸã€‚å…¥åŠ›å†…å®¹ï¼ˆè³ªå•ãƒ»ç”»åƒãƒ»PDFï¼‰ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
            return

        # ---- 5) è§£æçµæœï¼ˆç·åˆè©•ä¾¡ã‚’å…ˆé ­ã«ï¼‰ ----
        st.markdown('<div class="md-card good-shadow jp-report">', unsafe_allow_html=True)
        st.markdown('<div class="md-title">5) è§£æçµæœ</div>', unsafe_allow_html=True)
        st.markdown('</div>', unsafe_allow_html=True)

        # ç·åˆè©•ä¾¡ãƒ–ãƒ­ãƒƒã‚¯æŠ½å‡º
        summary_block = None
        try:
            pattern = r"(?:^|\n)##?\s*ç·åˆè©•ä¾¡[\s\S]*?(?=\n##?\s|\Z)"
            m = re.search(pattern, report_md, re.IGNORECASE)
            if m:
                summary_block = m.group(0)
        except Exception:
            summary_block = None

        if summary_block:
            st.markdown('<div class="md-card good-shadow jp-report">', unsafe_allow_html=True)
            st.markdown('<div class="md-title">ğŸ§­ ç·åˆè©•ä¾¡ï¼ˆè¦ç´„ï¼‰</div>', unsafe_allow_html=True)
            st.markdown(f"<div class='jp-report'>{summary_block}</div>", unsafe_allow_html=True)
            st.markdown('</div>', unsafe_allow_html=True)

        # å…¨æ–‡ï¼ˆè©³ç´°ï¼‰ã¯Expanderå†…
        with st.expander("è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤º", expanded=(summary_block is None)):
            st.markdown(f"<div class='jp-report'>{report_md}</div>", unsafe_allow_html=True)

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå…±æœ‰ï¼‰
        st.download_button(
            "ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            report_md,
            file_name="building_health_report.md",
            mime="text/markdown",
            use_container_width=True
        )

    # ãƒ•ãƒƒã‚¿
    st.markdown("")
    st.caption("Â© å»ºç‰©è¨ºæ–­ãã‚“ â€” å¯è¦–/èµ¤å¤– Ã— RAGï¼ˆBM25/ãƒ–ãƒ¼ã‚¹ãƒˆï¼‰Ã— Gemini 2.5 Flashã€‚ãƒ¢ãƒã‚¤ãƒ«æœ€é©åŒ–UIã€‚")


if __name__ == "__main__":
    main()
