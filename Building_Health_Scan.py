import streamlit as st
import requests
import faiss
import numpy as np
import PyPDF2
from PIL import Image
from sentence_transformers import SentenceTransformer
from transformers import BlipProcessor, BlipForConditionalGeneration

# -------------------------------
# PDFå‡¦ç†é–¢é€£é–¢æ•°
# -------------------------------

def extract_text_from_pdf(pdf_path):
    text = ""
    try:
        with open(pdf_path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            if reader.is_encrypted:
                st.warning(f"PDFãƒ•ã‚¡ã‚¤ãƒ« '{pdf_path}' ã¯æš—å·åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚")
                return ""
            for page in reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text + "\n"
    except FileNotFoundError:
        st.error(f"PDFãƒ•ã‚¡ã‚¤ãƒ« '{pdf_path}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
    except PyPDF2.errors.PdfReadError as e:
        st.error(f"PDFèª­ã¿å–ã‚Šã‚¨ãƒ©ãƒ¼: {e}")
    return text

def split_text_into_chunks(text, max_length=500):
    return [text[i:i+max_length] for i in range(0, len(text), max_length)]

def create_vector_index(chunks, model):
    embeddings = model.encode(chunks, convert_to_numpy=True, batch_size=32)
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)
    return index

@st.cache_resource(show_spinner=False)
def load_pdf_index():
    pdf_paths = [
        "Structure_Base.pdf",
        "kamijimachou_Public_facility_management_plan.pdf",
        "minatoku_Public_facility_management_plan.pdf"
    ]
    combined_text = ""
    for path in pdf_paths:
        combined_text += extract_text_from_pdf(path) + "\n"
    chunks = split_text_into_chunks(combined_text)
    chunks = [chunk for chunk in chunks if chunk.strip()]
    vec_model = SentenceTransformer('all-MiniLM-L6-v2')
    index = create_vector_index(chunks, vec_model)
    return index, chunks, vec_model

def search_relevant_chunks(query, model, index, chunks, top_k=3):
    query_vec = model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_vec, top_k)
    return [chunks[i] for i in indices[0] if i < len(chunks)]

# -------------------------------
# ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆé–¢é€£é–¢æ•°
# -------------------------------

@st.cache_resource(show_spinner=False)
def load_caption_model():
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
    model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
    return processor, model

def generate_image_caption(image, processor, model):
    if image.width > 800:
        image = image.resize((800, int(image.height * 800 / image.width)))
    if image.mode != 'RGB':
        image = image.convert('RGB')

    prompt = (
        "ç”»åƒå†…ã®å£ã¨åºŠã®çŠ¶æ…‹ã‚’è©³ã—ãæ—¥æœ¬èªã§èª¬æ˜ã—ã¦ãã ã•ã„ã€‚"
        "ã²ã³å‰²ã‚Œã€æ±šã‚Œã€è†¨ã‚‰ã¿ã€å‰¥ãŒã‚Œã€å¤‰è‰²ãªã©ã®æå‚·ã‚’å«ã‚ã€å»ºç‰©æ¤œæŸ»å®˜ã®è¦–ç‚¹ã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚"
    )
    inputs = processor([image], text=prompt, return_tensors="pt", padding=True)
    output = model.generate(**inputs)
    return processor.decode(output[0], skip_special_tokens=True)

# -------------------------------
# Gemini API å‘¼ã³å‡ºã—é–¢é€£é–¢æ•°
# -------------------------------

def generate_report_with_gemini(prompt_text):
    try:
        api_key = st.secrets["gemini"]["API_KEY"]
    except KeyError:
        st.error("Gemini APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return None

    endpoint = (
        "https://generativelanguage.googleapis.com/v1beta/models/"
        "gemini-1.5-flash-latest:generateContent?key=" + api_key
    )

    payload = {"contents": [{"parts": [{"text": prompt_text}]}]}
    try:
        response = requests.post(endpoint, headers={"Content-Type": "application/json"}, json=payload)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        st.error(f"APIãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
        return None

# -------------------------------
# Streamlit UIéƒ¨åˆ†
# -------------------------------

st.set_page_config(page_title="å£ãƒ»åºŠçŠ¶æ…‹åˆ†æãƒ„ãƒ¼ãƒ«", layout="wide")
st.title("ğŸ§± å£ãƒ»åºŠçŠ¶æ…‹åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ„ãƒ¼ãƒ«")

st.markdown("""
å»ºç‰©ã®å£ã‚„åºŠã®çŠ¶æ…‹ã‚’AIãŒè§£æã—ã€è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚  
ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã‹ã€ã‚«ãƒ¡ãƒ©ã§æ’®å½±ã—ã¦ãã ã•ã„ã€‚
""")

# ç”»åƒå…¥åŠ›ã‚»ã‚¯ã‚·ãƒ§ãƒ³
image = None
with st.expander("ğŸ“¸ ç”»åƒã®å…¥åŠ›æ–¹æ³•ã‚’é¸æŠ"):
    image_file = st.file_uploader("ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["jpg", "jpeg", "png"])
    camera_image = st.camera_input("ã¾ãŸã¯ã‚«ãƒ¡ãƒ©ã§æ’®å½±")

    if image_file:
        image = Image.open(image_file)
    elif camera_image:
        image = Image.open(camera_image)

    if image:
        st.image(image, caption="ä½¿ç”¨ã™ã‚‹ç”»åƒ", use_column_width=True)

# è£œè¶³æƒ…å ±å…¥åŠ›
with st.expander("ğŸ“ è£œè¶³æƒ…å ±ã‚’å…¥åŠ›ï¼ˆä»»æ„ï¼‰"):
    wall_note = st.text_area("å£ã«é–¢ã™ã‚‹è£œè¶³æƒ…å ±ï¼ˆã²ã³å‰²ã‚Œã€å¤‰è‰²ãªã©ï¼‰")
    floor_note = st.text_area("åºŠã«é–¢ã™ã‚‹è£œè¶³æƒ…å ±ï¼ˆãã—ã¿ã€æ²ˆã¿ãªã©ï¼‰")
    user_query = st.text_input("ç‰¹ã«è³ªå•ã—ãŸã„å†…å®¹")

# ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
if st.button("ğŸ“Š ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹"):
    if not image:
        st.error("ç”»åƒã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    processor, cap_model = load_caption_model()

    with st.spinner("ç”»åƒã‚’è§£æä¸­â€¦"):
        caption = generate_image_caption(image, processor, cap_model)
    st.success("ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚")
    st.write(caption)

    index, chunks, vec_model = load_pdf_index()
    relevant_chunks = search_relevant_chunks(user_query, vec_model, index, chunks)
    context = "\n\n".join(relevant_chunks)

    prompt = (
        f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {user_query}\n\n"
        f"é–¢é€£æƒ…å ±:\n{context}\n\n"
        f"ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³:\n{caption}\n\n"
        f"å£ã®è£œè¶³æƒ…å ±:\n{wall_note}\n\n"
        f"åºŠã®è£œè¶³æƒ…å ±:\n{floor_note}\n\n"
        "ä¸Šè¨˜ã‚’åŸºã«ã€å£ã¨åºŠã®çŠ¶æ…‹ã‚’åˆ†æã—ã€ä»¥ä¸‹ã®Markdownå½¢å¼ã§ãƒ¬ãƒãƒ¼ãƒˆä½œæˆ:\n"
        "## 1. å£ã®çŠ¶æ…‹åˆ†æ\n"
        "### 1.1 ç¾çŠ¶\n(å£ã®ç¾çŠ¶)\n"
        "### 1.2 åŠ£åŒ–åº¦ (Aï½Dã§è©•ä¾¡)\n"
        "### 1.3 æ¨å®šå¯¿å‘½\n"
        "### 1.4 å¿…è¦ãªå¯¾ç­–\n"
        "### 1.5 æ³¨æ„ç‚¹\n"
        "## 2. åºŠã®çŠ¶æ…‹åˆ†æ\n"
        "### 2.1 ç¾çŠ¶\n(åºŠã®ç¾çŠ¶)\n"
        "### 2.2 åŠ£åŒ–åº¦ (Aï½Dã§è©•ä¾¡)\n"
        "### 2.3 æ¨å®šå¯¿å‘½\n"
        "### 2.4 å¿…è¦ãªå¯¾ç­–\n"
        "### 2.5 æ³¨æ„ç‚¹\n"
        "æƒ…å ±ä¸è¶³æ™‚ã¯å°‚é–€çš„ãªçŸ¥è­˜ã‚„ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆæ¤œç´¢ã‚‚ä½µç”¨ã—ã€ç´ äººã«ã‚‚ç†è§£ã—ã‚„ã™ãç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚"
    )

    with st.spinner("ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆä¸­â€¦"):
        result = generate_report_with_gemini(prompt)
        if result:
            report_text = result["candidates"][0]["content"]["parts"][0]["text"]
            st.success("ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†ï¼")

            for section in report_text.split("## "):
                if section.strip():
                    header, content = section.split("\n", 1)
                    with st.expander(header.strip(), expanded=True):
                        st.markdown(content.strip())

            st.download_button("ğŸ“¥ ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", report_text, "report.md")
        else:
            st.error("ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
