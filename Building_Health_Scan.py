# -*- coding: utf-8 -*-
# ===========================================================
# å»ºç‰©è¨ºæ–­ãã‚“ï¼ˆã‚¹ãƒãƒ›å¯¾å¿œãƒ»ãƒãƒ†ãƒªã‚¢ãƒ«UIï¼‰
# RAGï¼ˆBM25+ãƒ–ãƒ¼ã‚¹ãƒˆï¼‰ï¼‹ ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ï¼ˆä¸€èˆ¬åŸå‰‡ï¼‰ï¼‹ ç”»åƒã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ‰€è¦‹ â†’ Gemini 2.5 Flash
# - å¯è¦–/èµ¤å¤–ï¼šã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ or ã‚«ãƒ¡ãƒ©ï¼ˆ1æšãšã¤ï¼‰
# - ç«¯æœ«ã®ç¾åœ¨åœ°ï¼ˆGeolocationï¼‰ï¼‹ Folium åœ°å›³
# - â€œçµæœã®ã¿â€ï¼šç·åˆè©•ä¾¡ã‚’æœ€åˆã«ã€è©³ç´°ã¯ Expander
# - CSSã¯ components.html ã§éè¡¨ç¤ºæŒ¿å…¥ï¼ˆç”ŸCSSãŒç”»é¢ã«å‡ºãªã„ï¼‰
# Python 3.12 äº’æ›ãƒ»è»½é‡ä¾å­˜
# ===========================================================

# Python 3.12: pkgutil.ImpImporterå‰Šé™¤å›é¿ï¼ˆå¤ã„ä¾å­˜å¯¾ç­–ï¼‰
import pkgutil
if not hasattr(pkgutil, "ImpImporter"):
    pkgutil.ImpImporter = pkgutil.zipimporter

import os
import io
import re
import math
import base64
import statistics
import unicodedata
from typing import List, Tuple, Dict, Optional, Any

import streamlit as st
import requests
import PyPDF2
from PIL import Image, ImageFilter

# åœ°å›³
import folium
from streamlit_folium import st_folium
from streamlit import components

# geolocationï¼ˆä»»æ„ï¼‰
HAVE_GEO = False
try:
    from streamlit_geolocation import st_geolocation
    HAVE_GEO = True
except Exception:
    HAVE_GEO = False

APP_TITLE = "å»ºç‰©è¨ºæ–­ãã‚“"

PDF_SOURCES = [
    ("Structure_Base.pdf", "Structure_Base.pdf"),
    ("ä¸Šå³¶ç”º å…¬å…±æ–½è¨­ç­‰ç·åˆç®¡ç†è¨ˆç”»", "kamijimachou_Public_facility_management_plan.pdf"),
    ("æ¸¯åŒº å…¬å…±æ–½è¨­ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆè¨ˆç”»", "minatoku_Public_facility_management_plan.pdf"),
]

MAX_SNIPPETS = 6
MAX_SNIPPET_CHARS = 900

QUERY_SYNONYMS = {
    "ã²ã³å‰²ã‚Œ": ["ã²ã³å‰²ã‚Œ", "ã‚¯ãƒ©ãƒƒã‚¯", "äº€è£‚", "ã²ã³"],
    "æµ®ã": ["æµ®ã", "ã†ã"],
    "å‰¥é›¢": ["å‰¥é›¢", "ã¯ãé›¢", "ã¯ãã‚Š"],
    "å«æ°´": ["å«æ°´", "æµ¸æ°´", "æ¼æ°´", "é›¨æ°´ä¾µå…¥"],
    "èµ¤å¤–ç·š": ["èµ¤å¤–ç·š", "IR", "ã‚µãƒ¼ãƒ¢", "ç†±ç”»åƒ", "ã‚µãƒ¼ãƒ¢ã‚°ãƒ©ãƒ•ã‚£"],
    "åŸºæº–": ["åŸºæº–", "åˆ¤å®š", "è©•ä¾¡", "é–¾å€¤"],
    "ã‚¿ã‚¤ãƒ«": ["ã‚¿ã‚¤ãƒ«", "ç£å™¨è³ªã‚¿ã‚¤ãƒ«", "ãƒ¢ã‚¶ã‚¤ã‚¯ã‚¿ã‚¤ãƒ«"],
    "ALC": ["ALC", "è»½é‡æ°—æ³¡ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆ"],
    "ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆ": ["ã‚³ãƒ³ã‚¯ãƒªãƒ¼ãƒˆ", "RC", "ä¸­æ€§åŒ–"],
    "é˜²æ°´": ["é˜²æ°´", "å¡—è†œ", "ã‚·ãƒ¼ãƒªãƒ³ã‚°", "ä¼¸ç¸®ç›®åœ°"],
    "åŠ£åŒ–åº¦": ["åŠ£åŒ–åº¦", "ã‚°ãƒ¬ãƒ¼ãƒ‰", "åŒºåˆ†", "A", "B", "C", "D"],
}

# -------------------------- ãƒ†ã‚­ã‚¹ãƒˆå‡¦ç† --------------------------
def normalize_text(text: str) -> str:
    text = unicodedata.normalize("NFKC", text)
    text = text.replace("\r", " ").replace("\n", " ")
    text = re.sub(r"\s+", " ", text).strip()
    return text

def tokenize(s: str) -> List[str]:
    s = s.lower()
    s = re.sub(r"[^\wä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³ï¼…â„ƒï¼\.]", " ", s)
    s = s.replace("ï¼", ".")
    s = re.sub(r"\s+", " ", s).strip()
    return s.split()

def query_expand_tokens(q: str) -> List[str]:
    tokens = set(tokenize(q))
    for key, syns in QUERY_SYNONYMS.items():
        if key in q or any(s in q for s in syns):
            for s in syns:
                tokens.update(tokenize(s))
    return list(tokens)

# -------------------------- PDF â†’ ãƒãƒ£ãƒ³ã‚¯ --------------------------
def extract_chunks_from_pdf(pdf_path: str, title: str,
                            max_chars: int = 900, overlap: int = 120) -> List[Dict[str, Any]]:
    if not os.path.exists(pdf_path):
        return []
    chunks: List[Dict[str, Any]] = []
    try:
        with open(pdf_path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            for i, page in enumerate(reader.pages, start=1):
                t = page.extract_text() or ""
                page_text = normalize_text(t)
                if not page_text:
                    continue
                pos = 0
                while pos < len(page_text):
                    end = min(pos + max_chars, len(page_text))
                    ch = page_text[pos:end].strip()
                    if ch:
                        chunks.append({
                            "doc": title,
                            "path": pdf_path,
                            "text": ch,
                            "page_start": i,
                            "page_end": i,
                        })
                    pos = end - overlap if (end - overlap) > pos else end
    except Exception as e:
        chunks.append({
            "doc": title, "path": pdf_path,
            "text": f"[PDFèª­è¾¼ã‚¨ãƒ©ãƒ¼:{pdf_path}:{e}]",
            "page_start": None, "page_end": None
        })
    return chunks

# -------------------------- BM25 ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ --------------------------
class BM25Index:
    def __init__(self, k1: float = 1.6, b: float = 0.75):
        self.k1, self.b = k1, b
        self.N = 0
        self.avgdl = 0.0
        self.df: Dict[str, int] = {}
        self.doc_len: List[int] = []
        self.postings: Dict[str, List[Tuple[int, int]]] = {}
        self.docs: List[Dict[str, Any]] = []

    @staticmethod
    def _tf(tokens: List[str]) -> Dict[str, int]:
        tf: Dict[str, int] = {}
        for t in tokens:
            tf[t] = tf.get(t, 0) + 1
        return tf

    def build(self, docs: List[Dict[str, Any]]):
        self.docs = docs
        self.N = len(docs)
        lengths = []
        for doc_id, d in enumerate(docs):
            tokens = tokenize(d["text"])
            d["_tokens"] = tokens
            tf = self._tf(tokens)
            lengths.append(len(tokens))
            for term, c in tf.items():
                self.df[term] = self.df.get(term, 0) + 1
                self.postings.setdefault(term, []).append((doc_id, c))
        self.doc_len = lengths
        self.avgdl = (sum(lengths) / self.N) if self.N else 0.0

    def idf(self, term: str) -> float:
        df = self.df.get(term, 0)
        return math.log((self.N - df + 0.5) / (df + 0.5) + 1.0)

    def score_doc(self, q_tokens: List[str], doc_id: int) -> float:
        score = 0.0
        dl = self.doc_len[doc_id] if doc_id < len(self.doc_len) else 0
        if dl == 0:
            return 0.0
        for term in set(q_tokens):
            plist = self.postings.get(term)
            if not plist:
                continue
            tf = 0
            for did, c in plist:
                if did == doc_id:
                    tf = c
                    break
            if tf == 0:
                continue
            idf = self.idf(term)
            denom = tf + self.k1 * (1 - self.b + self.b * (dl / (self.avgdl or 1.0)))
            score += idf * (tf * (self.k1 + 1)) / (denom or 1.0)
        return score

@st.cache_resource(show_spinner=False)
def build_rag() -> Dict[str, Any]:
    all_chunks: List[Dict[str, Any]] = []
    for title, path in PDF_SOURCES:
        all_chunks.extend(extract_chunks_from_pdf(path, title))
    # è¿½åŠ ãƒ¡ã‚¿
    for d in all_chunks:
        txt = d["text"]
        d["_has_numbers"] = bool(re.search(r"\b\d+(\.\d+)?\s*(mm|ãœ|ï¼…|%|â„ƒ)\b", txt))
        d["_has_ir"] = any(k in txt for k in ["èµ¤å¤–ç·š", "ã‚µãƒ¼ãƒ¢", "IR", "ç†±ç”»åƒ", "æ”¾å°„ç‡", "åå°„æ¸©åº¦"])
    bm25 = BM25Index()
    if all_chunks:
        bm25.build(all_chunks)
    return {"index": bm25, "docs": all_chunks}

def rag_search(query: str, have_ir: bool, k: int = MAX_SNIPPETS) -> List[Dict[str, Any]]:
    data = build_rag()
    bm25: BM25Index = data["index"]
    docs = data["docs"]
    if not docs:
        return []
    q_tokens = query_expand_tokens(query)
    want_threshold = any(w in query for w in ["åŸºæº–", "é–¾å€¤", "å¹…", "mm", "ï¼…", "%", "â„ƒ", "æ¸©åº¦"])
    boost_muni = "æ¸¯åŒº" if "æ¸¯åŒº" in query else ("ä¸Šå³¶ç”º" if ("ä¸Šå³¶ç”º" in query or "ä¸Šå¶‹ç”º" in query) else None)

    scored: List[Tuple[float, int]] = []
    for doc_id, d in enumerate(docs):
        base = bm25.score_doc(q_tokens, doc_id)
        if want_threshold and d.get("_has_numbers"):
            base *= 1.12
        if have_ir and d.get("_has_ir"):
            base *= 1.10
        if boost_muni and boost_muni in d["doc"]:
            base *= 1.15
        if base > 0:
            scored.append((base, doc_id))
    scored.sort(key=lambda x: x[0], reverse=True)
    top = [docs[i] for (_, i) in scored[:k]]
    for t in top:
        if len(t["text"]) > MAX_SNIPPET_CHARS:
            t["text"] = t["text"][:MAX_SNIPPET_CHARS] + "â€¦"
    return top

# ---------------------- ç”»åƒã®è»½é‡ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚° ----------------------
def pil_stats(image: Image.Image) -> Dict[str, float]:
    """PILã ã‘ã§è»½é‡ã«ç”»ç´ çµ±è¨ˆã‚’å–ã‚‹ï¼ˆãƒªã‚µã‚¤ã‚ºã—ã¦è² è·è»½æ¸›ï¼‰"""
    if image.mode != "RGB":
        image = image.convert("RGB")
    # å°ã•ãã—ã¦ã‹ã‚‰çµ±è¨ˆ
    w = 256 if image.width > 256 else image.width
    ratio = w / image.width
    image = image.resize((w, int(image.height * ratio)))
    # æ˜åº¦
    gray = image.convert("L")
    pixels = list(gray.getdata())
    mean_l = statistics.fmean(pixels) if pixels else 0.0
    stdev_l = statistics.pstdev(pixels) if pixels else 0.0
    # ã‚¨ãƒƒã‚¸é‡ï¼ˆç°¡æ˜“ï¼‰
    edges = gray.filter(ImageFilter.FIND_EDGES)
    epx = list(edges.getdata())
    # å¼·ã„ã‚¨ãƒƒã‚¸ã®å‰²åˆï¼ˆ0â€“255 ã®ã†ã¡ > 60 ã‚’å¼·ã‚¨ãƒƒã‚¸ã¨ã¿ãªã™ç°¡æ˜“æŒ‡æ¨™ï¼‰
    strong = sum(1 for v in epx if v > 60)
    edge_ratio = strong / len(epx) if epx else 0.0
    # å½©åº¦
    hsv = image.convert("HSV")
    sat_vals = [p[1] for p in list(hsv.getdata())]
    sat_mean = statistics.fmean(sat_vals) if sat_vals else 0.0
    return {
        "mean_l": round(mean_l, 2),
        "stdev_l": round(stdev_l, 2),
        "edge_ratio": round(edge_ratio, 4),
        "sat_mean": round(sat_mean, 2),
    }

def analyze_visual(image: Image.Image) -> Dict[str, Any]:
    """å¯è¦–ç”»åƒã®ç°¡æ˜“æ‰€è¦‹ï¼ˆã²ã³å‚¾å‘/æ±šã‚Œå‚¾å‘ãªã©ã®ã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼‰"""
    s = pil_stats(image)
    # ã¨ã¦ã‚‚ç´ æœ´ãªãƒ«ãƒ¼ãƒ«ç¾¤ï¼ˆå®‰å…¨å´ï¼‰ï¼šã‚¨ãƒƒã‚¸å¯†åº¦ãŒé«˜ã„ã¨â€œã‚¯ãƒ©ãƒƒã‚¯å‚¾å‘â€
    crack_hint = s["edge_ratio"] > 0.11
    stain_hint = s["sat_mean"] < 70 and s["mean_l"] < 110  # ä½å½©åº¦ã‹ã¤æš—ã‚â†’æ±šæŸ“/é›¨ã ã‚Œå‚¾å‘ã®å¯èƒ½æ€§
    level = "low"
    if crack_hint and stain_hint:
        level = "mid"
    if s["edge_ratio"] > 0.16:
        level = "mid"
    if s["edge_ratio"] > 0.22:
        level = "high"
    return {
        "metrics": s,
        "crack_hint": crack_hint,
        "stain_hint": stain_hint,
        "screening_level": level,
        "note": "ç”»åƒãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“å‚¾å‘ã€‚å¯¸æ³•ãƒ»å¹…ãªã©ã¯ç”»åƒã ã‘ã§ã¯ç¢ºå®šã§ãã¾ã›ã‚“ã€‚"
    }

def analyze_ir(image: Image.Image, meta: Dict[str, str]) -> Dict[str, Any]:
    """IRç”»åƒã®ç›¸å¯¾æ¸©åº¦å·®ï¼ˆè¼åº¦å·®ï¼‰ã‚’ç°¡æ˜“è©•ä¾¡ï¼ˆJPEGã®ç›¸å¯¾æŒ‡æ¨™ã€‚çµ¶å¯¾æ¸©åº¦ã¯æ‰±ã‚ãªã„ï¼‰"""
    gray = image.convert("L")
    w = 256 if gray.width > 256 else gray.width
    gray = gray.resize((w, int(gray.height * (w / gray.width))))
    vals = list(gray.getdata())
    if not vals:
        return {"has_ir": True, "delta_rel": 0.0, "pattern": "unknown", "note": "ç”»åƒãƒ‡ãƒ¼ã‚¿ç„¡ã—"}
    vmin, vmax = min(vals), max(vals)
    delta = vmax - vmin  # 0â€“255
    stdev = statistics.pstdev(vals)
    # ç›¸å¯¾è©•ä¾¡
    pattern = "uniform"
    if stdev > 20 and delta > 40:
        pattern = "hot/cold spots"
    if stdev > 30 and delta > 60:
        pattern = "strong hotspots"
    return {
        "has_ir": True,
        "delta_rel": round(delta / 255.0, 3),  # 0â€“1 ã®ç›¸å¯¾å·®æŒ‡æ¨™
        "stdev": round(stdev, 2),
        "pattern": pattern,
        "meta": meta,
        "note": "JPEGã®è¼åº¦å·®ã‹ã‚‰è¦‹ãŸç›¸å¯¾çš„ãªãƒ ãƒ©è©•ä¾¡ã€‚æ”¾å°„ç‡/åå°„æ¸©åº¦ç­‰ã®è£œæ­£ç„¡ã—ã€‚"
    }

# ---------------------- ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹åˆæˆï¼ˆå®‰å…¨å´ï¼‰ ----------------------
def rule_based_grade(vis: Optional[Dict[str, Any]], ir: Optional[Dict[str, Any]]) -> Tuple[str, str]:
    """
    ç”»åƒã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ‰€è¦‹ã‹ã‚‰ Aâ€“D ã®æš«å®šã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’æ¨å®šï¼ˆå®‰å…¨å´ï¼‰ã€‚
    - å¯è¦–ã®ã‚¨ãƒƒã‚¸æ¯”/æ±šã‚Œå‚¾å‘
    - IRã®ãƒ ãƒ©ï¼ˆdelta_rel, patternï¼‰
    """
    score = 0.0
    reasons = []
    if vis:
        er = vis["metrics"]["edge_ratio"]
        if er > 0.22:
            score += 3; reasons.append(f"å¼·ã„ã‚¨ãƒƒã‚¸å¯†åº¦ï¼ˆedge_ratio={er}ï¼‰")
        elif er > 0.16:
            score += 2; reasons.append(f"é«˜ã‚ã®ã‚¨ãƒƒã‚¸å¯†åº¦ï¼ˆedge_ratio={er}ï¼‰")
        elif er > 0.11:
            score += 1; reasons.append(f"ã‚„ã‚„é«˜ã„ã‚¨ãƒƒã‚¸å¯†åº¦ï¼ˆedge_ratio={er}ï¼‰")
        if vis.get("stain_hint"):
            score += 1; reasons.append("ä½å½©åº¦ãƒ»æš—éƒ¨å¤šã‚ã§æ±šæŸ“å‚¾å‘")
    if ir and ir.get("has_ir"):
        dr = ir["delta_rel"]
        if dr > 0.5:
            score += 3; reasons.append(f"IRè¼åº¦å·®ãŒå¤§ï¼ˆÎ”ç›¸å¯¾={dr}ï¼‰")
        elif dr > 0.3:
            score += 2; reasons.append(f"IRè¼åº¦å·®ãŒä¸­ï¼ˆÎ”ç›¸å¯¾={dr}ï¼‰")
        elif dr > 0.15:
            score += 1; reasons.append(f"IRè¼åº¦å·®ãŒã‚„ã‚„å¤§ï¼ˆÎ”ç›¸å¯¾={dr}ï¼‰")
        if ir.get("pattern") in ("strong hotspots",):
            score += 1; reasons.append("å¼·ã„ãƒ›ãƒƒãƒˆã‚¹ãƒãƒƒãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³")
    # ã‚¹ã‚³ã‚¢ â†’ ã‚°ãƒ¬ãƒ¼ãƒ‰
    if score <= 1:
        g = "A"
    elif score <= 3:
        g = "B"
    elif score <= 5:
        g = "C"
    else:
        g = "D"
    return g, "ãƒ»".join(reasons) if reasons else "é¡•è‘—ãªç•°å¸¸ã¯æ¤œå‡ºã•ã‚Œãšï¼ˆç”»åƒãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“åˆ¤å®šï¼‰"

def rule_based_life(grade: str) -> str:
    """å‚è€ƒã®å¯¿å‘½å¹…ï¼ˆç”»åƒã¨ä¸€èˆ¬åŸå‰‡ã®æš«å®šã€‚RAGã®æ ¹æ‹ ãŒã‚ã‚Œã°LLMå´ã§ä¸Šæ›¸ãï¼‰"""
    mapping = {
        "A": "10â€“20å¹´ï¼ˆå‚è€ƒï¼‰",
        "B": "7â€“15å¹´ï¼ˆå‚è€ƒï¼‰",
        "C": "3â€“10å¹´ï¼ˆå‚è€ƒï¼‰",
        "D": "1â€“5å¹´ï¼ˆå‚è€ƒï¼‰",
    }
    return mapping.get(grade, "æœªè©•ä¾¡")

# ---------------------- ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ï¼ˆä¸€èˆ¬åŸå‰‡ï¼‰ ----------------------
def domain_priors_text() -> str:
    """
    å®‰å…¨å´ã®ä¸€èˆ¬åŸå‰‡ã ã‘ã‚’åˆ—æŒ™ï¼ˆå…·ä½“çš„é–¾å€¤ã¯RAGã«ç„¡ã„é™ã‚Šä½¿ã‚ãªã„ï¼‰
    """
    return normalize_text("""
- å†™çœŸã®ã¿ã‹ã‚‰å¯¸æ³•ï¼ˆã²ã³å¹…ç­‰ï¼‰ã‚„ææ–™ç‰¹æ€§ã‚’æ­£ç¢ºã«æ±ºå®šã™ã‚‹ã“ã¨ã¯ã§ããªã„ã€‚æ•°å€¤ãŒå¿…è¦ãªå ´åˆã¯æ ¹æ‹ æ–‡çŒ®ã®é–¾å€¤ã‚’å¼•ç”¨ã™ã‚‹ã€‚
- å¯è¦–ï¼šã‚¯ãƒ©ãƒƒã‚¯/ç›®åœ°åŠ£åŒ–ã¯é›¨æ°´æµ¸å…¥ãƒ»ä»•ä¸Šã’å‰¥é›¢ãƒ»èº¯ä½“åŠ£åŒ–ã«ã¤ãªãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
- èµ¤å¤–ç·šï¼šæ”¾å°„ç‡ãƒ»åå°„æ¸©åº¦ãƒ»å¤–æ°—æ¸©ãƒ»æ¹¿åº¦ãƒ»æ’®å½±è·é›¢/è§’åº¦ãƒ»é¢¨/æ—¥å°„ç­‰ã®æ¡ä»¶ã«å¼·ãå½±éŸ¿ã•ã‚Œã‚‹ã€‚JPEGã®ç–‘ä¼¼æ¸©åº¦ã¯çµ¶å¯¾å€¤ã¨ã—ã¦æ‰±ã‚ãªã„ã€‚
- å®‰å…¨ç¢ºä¿ï¼šå‰¥é›¢ãƒ»è½ä¸‹ãƒªã‚¹ã‚¯ãŒã‚ã‚‹å ´åˆã¯è¿‘æ¥èª¿æŸ»ï¼ˆæ‰“è¨º/ã¯ã¤ã‚Š/ä»˜ç€åŠ›ç­‰ï¼‰ã¨æ—©æœŸä»®è¨­é˜²è­·ã‚’æ¤œè¨ã™ã‚‹ã€‚
- ç¶­æŒç®¡ç†ï¼šè»½å¾®æ®µéšã§ã®ç›®åœ°/é˜²æ°´è£œä¿®ãƒ»å†å¡—è£…ç­‰ãŒãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚³ã‚¹ãƒˆã‚’ä½æ¸›ã™ã‚‹ã€‚
- åˆ¤å®šã¯ã€Œç”»åƒæ‰€è¦‹ã€ã€ŒRAGæ ¹æ‹ ã€ã€Œç¾åœ°æ¡ä»¶ã€ã®æ•´åˆã§è¡Œã„ã€ä¸è¶³ãŒã‚ã‚Œã°â€œæœªç¢ºå®šâ€ã¨è¡¨ç¾ã™ã‚‹ã€‚
    """)

# ---------------------- Gemini API ----------------------
def image_to_inline_part(image: Image.Image, max_width: int = 1400) -> Dict:
    if image.mode != "RGB":
        image = image.convert("RGB")
    if image.width > max_width:
        r = max_width / float(image.width)
        image = image.resize((max_width, int(image.height * r)))
    buf = io.BytesIO()
    image.save(buf, format="JPEG", quality=90, optimize=True)
    b64 = base64.b64encode(buf.getvalue()).decode("utf-8")
    return {"inline_data": {"mime_type": "image/jpeg", "data": b64}}

def call_gemini(api_key: str, prompt_text: str, image_parts: List[Dict]) -> Dict:
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash:generateContent?key={api_key}"
    headers = {"Content-Type": "application/json"}
    parts = [{"text": prompt_text}]
    parts.extend(image_parts)
    payload = {"contents": [{"parts": parts}]}
    resp = requests.post(url, headers=headers, json=payload, timeout=120)
    resp.raise_for_status()
    return resp.json()

def extract_text_from_gemini(result: Dict) -> str:
    try:
        return result["candidates"][0]["content"]["parts"][0]["text"]
    except Exception:
        return ""

# ---------------------- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ„ç«‹ ----------------------
def build_master_prompt(user_q: str,
                        rag_snippets: List[Dict[str, Any]],
                        priors: str,
                        vis_find: Optional[Dict[str, Any]],
                        ir_find: Optional[Dict[str, Any]],
                        rule_grade: str,
                        rule_life: str,
                        ir_meta: Dict) -> str:
    # RAGæŠœç²‹
    rag_lines = []
    for d in rag_snippets:
        pg = ""
        if d.get("page_start") is not None:
            pg = f" p.{d['page_start']}"
        rag_lines.append(f"[{d['doc']}{pg}] {d['text']}")
    rag_text = "\n".join(rag_lines) if rag_lines else "ï¼ˆè©²å½“æŠœç²‹ãªã—ï¼‰"

    vis_block = "" if not vis_find else f"""å¯è¦–ç”»åƒæ‰€è¦‹(ç°¡æ˜“):
- metrics: {vis_find['metrics']}
- crack_hint: {vis_find['crack_hint']}
- stain_hint: {vis_find['stain_hint']}
- screening_level: {vis_find['screening_level']}
"""
    ir_block = ""
    if ir_find:
        ir_block = f"""IRç”»åƒæ‰€è¦‹(ç›¸å¯¾):
- delta_rel(0-1): {ir_find['delta_rel']}, stdev: {ir_find['stdev']}, pattern: {ir_find['pattern']}
- æ³¨æ„: {ir_find['note']}
- ãƒ¡ã‚¿: Îµ={ir_meta.get('emissivity','ä¸æ˜')}, T_ref={ir_meta.get('t_ref','ä¸æ˜')}â„ƒ, T_amb={ir_meta.get('t_amb','ä¸æ˜')}â„ƒ, RH={ir_meta.get('rh','ä¸æ˜')}%, dist={ir_meta.get('dist','ä¸æ˜')}m, angle={ir_meta.get('angle','ä¸æ˜')}Â°
"""

    prompt = f"""
ã‚ãªãŸã¯éç ´å£Šæ¤œæŸ»ãƒ»å»ºç¯‰ãƒ»ææ–™å­¦ã®ä¸Šç´šè¨ºæ–­å£«ã€‚
ä»¥ä¸‹ã®ææ–™ã ã‘ã‚’æ ¹æ‹ ã«ã€æ—¥æœ¬èªã§â€œçµæœã®ã¿â€ã®çŸ­ã„ãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã›ã‚ˆã€‚
**ç¦æ­¢**ï¼šæ¨æ¸¬ã§ã®æ•°å€¤åŒ–ãƒ»æœªå‡ºå…¸ã®é–¾å€¤è¨˜è¼‰ãƒ»ä¸€èˆ¬çŸ¥è­˜ã®ç„¡æ ¹æ‹ å¼•ç”¨ã€‚æ ¹æ‹ ãŒç„¡ã„é …ç›®ã¯ã€Œæœªæ²è¼‰/æœªç¢ºå®šã€ã¨ã™ã‚‹ã€‚

# å…¥åŠ›
- ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•: {user_q}

- RAGæŠœç²‹ï¼ˆå‡ºå…¸ãƒšãƒ¼ã‚¸ä»˜ãã€‚ã“ã‚Œä»¥å¤–ã‚’æ ¹æ‹ ã«ã—ãªã„ï¼‰:
{rag_text}

- ãƒ‰ãƒ¡ã‚¤ãƒ³ä¸€èˆ¬åŸå‰‡ï¼ˆæ–¹é‡ãƒ»æ³¨æ„ç‚¹ã€‚é–¾å€¤ã¯å«ã¾ãªã„ï¼‰:
{priors}

- ç”»åƒã‚¹ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°æ‰€è¦‹:
{vis_block}{ir_block}

- ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹æš«å®š:
  * æš«å®šã‚°ãƒ¬ãƒ¼ãƒ‰: {rule_grade}
  * å‚è€ƒå¯¿å‘½: {rule_life}

# å‡ºåŠ›ä»•æ§˜ï¼ˆMarkdownã€â€œçµæœã®ã¿â€ï¼‰
1. **ç·åˆè©•ä¾¡**ï¼ˆA/B/C/Dã€ä¸»å› 1â€“2è¡Œã€æš«å®šã‚’æ˜è¨˜å¯ï¼‰
2. **æ¨å®šæ®‹å­˜å¯¿å‘½**ï¼ˆå¹…ã€‚RAGã«æ ¹æ‹ ãŒç„¡ã‘ã‚Œã°ã€Œå‚è€ƒï¼ˆç”»åƒãƒ»ä¸€èˆ¬åŸå‰‡ãƒ™ãƒ¼ã‚¹ï¼‰ã€ã¨æ˜ç¤ºï¼‰
3. **ä¸»è¦æ ¹æ‹ **ï¼ˆå¯è¦–/IR/RAGã®é †ã§ç°¡æ½”ã€‚å„é …ç›®ã« [å‡ºå…¸] ã‚’ä»˜ä¸ã€‚æ•°å€¤ã¯RAGã«ã‚ã‚‹å ´åˆã®ã¿ï¼‰
4. **æ¨å¥¨å¯¾å¿œï¼ˆå„ªå…ˆåº¦é †ï¼‰**ï¼ˆå®‰å…¨å´ã€‚å‡ºå…¸ãŒã‚ã‚Œã°ä½µè¨˜ï¼‰
5. **é™ç•Œã¨è¿½åŠ ãƒ‡ãƒ¼ã‚¿è¦æœ›**ï¼ˆä¸è¶³æ ¹æ‹ ã€å¿…è¦ãªç¾åœ°ç¢ºèªãƒ»NDTï¼‰

æ³¨æ„ï¼šç”»åƒã®ã¿ã§ç¢ºå®šã§ããªã„é …ç›®ã¯æ–­å®šã—ãªã„ã€‚IR JPEGã¯ç›¸å¯¾åˆ¤æ–­ã«ç•™ã‚ã‚‹ã€‚
""".strip()
    return normalize_text(prompt)

# ---------------------- CSSï¼ˆè¡¨ç¤ºã•ã‚Œãªã„æ³¨å…¥ï¼‰ ----------------------
def inject_material_css():
    components.v1.html(
        """
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@400;500;700;900&family=Noto+Serif+JP:wght@500;700&display=swap" rel="stylesheet">
<style id="app-style">
:root{
  --mdc-primary:#2962ff; --mdc-secondary:#00b8d4;
  --mdc-bg:#f7f9fc; --mdc-surface:#ffffff; --mdc-outline:rgba(0,0,0,.08);
  --radius:16px; --shadow:0 6px 18px rgba(0,0,0,.08); --tap-min:44px;
}
@media (prefers-color-scheme: dark){
  :root{ --mdc-bg:#0f1115; --mdc-surface:#171a21; --mdc-outline:rgba(255,255,255,.08); }
}
.block-container{padding-top:2.6rem !important;padding-bottom:2rem;}
body{background:var(--mdc-bg);}
.jp-sans{font-family:'Noto Sans JP',system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI','Hiragino Kaku Gothic ProN','Hiragino Sans','Meiryo',sans-serif!important;line-height:1.7;}
.jp-report *{font-family:'Noto Sans JP',system-ui,-apple-system,BlinkMacSystemFont,'Segoe UI','Hiragino Kaku Gothic ProN','Hiragino Sans','Meiryo',sans-serif!important;line-height:1.6;}
.app-hero{background:linear-gradient(135deg,var(--mdc-primary),var(--mdc-secondary));color:#fff;border-radius:20px;box-shadow:var(--shadow);padding:14px 16px;margin:0 0 14px 0;}
.app-hero-title{font-weight:900;font-size:1.45rem;line-height:1.25;margin:0 0 4px 0;text-shadow:0 1px 2px rgba(0,0,0,.18);}
.app-hero-sub{font-weight:500;font-size:.95rem;opacity:.95;margin:0;}
.md-card{background:var(--mdc-surface);border-radius:var(--radius);box-shadow:var(--shadow);padding:1rem 1.1rem;margin:0 0 1rem 0;border:1px solid var(--mdc-outline);}
.md-title{font-size:1.1rem;font-weight:700;margin:0 0 .6rem 0;}
.stButton>button,.stTextInput input,.stFileUploader label,.stCameraInput label{min-height:var(--tap-min);border-radius:12px!important;font-weight:600;}
.stButton>button{background:linear-gradient(135deg,var(--mdc-primary),var(--mdc-secondary))!important;color:#fff!important;border:none!important;box-shadow:var(--shadow);}
:where(button,input,select,textarea):focus-visible{outline:3px solid color-mix(in srgb,var(--mdc-primary) 60%, white);outline-offset:2px;border-radius:12px;}
</style>
        """,
        height=0,
    )

# ---------------------- ä½ç½®ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯JSï¼‰ ----------------------
def geolocate_fallback_via_query_params(show_widget: bool = True) -> Tuple[Optional[float], Optional[float]]:
    params = st.experimental_get_query_params()
    lat = params.get("lat", [None])[0]
    lon = params.get("lon", [None])[0]
    if lat and lon:
        try:
            return float(lat), float(lon)
        except Exception:
            return None, None
    if show_widget:
        components.v1.html(
            """
<script>
(function(){
  if (!navigator.geolocation){return;}
  navigator.geolocation.getCurrentPosition(function(pos){
    const lat = pos.coords.latitude.toFixed(6);
    const lon = pos.coords.longitude.toFixed(6);
    const url = new URL(window.location.href);
    url.searchParams.set('lat', lat);
    url.searchParams.set('lon', lon);
    const a=document.createElement('a'); a.href=url.toString(); a.target='_top'; document.body.appendChild(a); a.click();
  }, function(){}, {enableHighAccuracy:true, timeout:10000, maximumAge:0});
})();
</script>
            """,
            height=0,
        )
    return None, None

# ---------------------- ãƒ¡ã‚¤ãƒ³ ----------------------
def main():
    st.set_page_config(page_title=APP_TITLE, layout="wide")
    inject_material_css()

    st.markdown(
        f"""
<div class="app-hero jp-sans">
  <div class="app-hero-title">ğŸ—ï¸ {APP_TITLE}</div>
  <div class="app-hero-sub">ã‚¹ãƒãƒ›æœ€é© / ç”»åƒï¼ˆå¯è¦–ãƒ»èµ¤å¤–ï¼‰Ã— RAG Ã— ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ Ã— Gemini 2.5 Flash</div>
</div>
        """,
        unsafe_allow_html=True
    )

    # 1) è³ªå•
    st.markdown('<div class="md-card">', unsafe_allow_html=True)
    st.markdown('<div class="md-title">1) è³ªå•</div>', unsafe_allow_html=True)
    user_q = st.text_input("ä¾‹ï¼šå¤–å£ã‚¿ã‚¤ãƒ«ã®ã²ã³å‰²ã‚ŒåŸºæº–ã¨æ¨å®šå¯¿å‘½", "", placeholder="åˆ†æã—ãŸã„ãƒ†ãƒ¼ãƒãƒ»è³ªå•ã‚’å…¥åŠ›")
    st.markdown('</div>', unsafe_allow_html=True)

    # 2) ç”»åƒ
    st.markdown('<div class="md-card">', unsafe_allow_html=True)
    st.markdown('<div class="md-title">2) ç”»åƒå…¥åŠ›ï¼ˆå¯è¦–/èµ¤å¤–ï¼‰</div>', unsafe_allow_html=True)
    colA, colB = st.columns(2)
    with colA:
        st.markdown("**å¯è¦–ï¼ˆã©ã¡ã‚‰ã‹1ã¤ï¼‰**")
        vis_file = st.file_uploader("å¯è¦–ç”»åƒï¼ˆJPG/PNGï¼‰", type=["jpg","jpeg","png"], key="vis_up")
        vis_cam  = st.camera_input("ã‚«ãƒ¡ãƒ©ã§æ’®å½±ï¼ˆå¯è¦–ï¼‰", key="vis_cam")
    with colB:
        st.markdown("**èµ¤å¤–ç·šï¼ˆIRï¼‰ç”»åƒï¼ˆä»»æ„ï¼‰**")
        ir_file = st.file_uploader("IRç”»åƒï¼ˆJPG/PNGï¼‰", type=["jpg","jpeg","png"], key="ir_up")
        with st.expander("IRãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆä»»æ„ãƒ»ç²¾åº¦å‘ä¸Šï¼‰"):
            ir_emiss = st.text_input("æ”¾å°„ç‡ Îµï¼ˆä¾‹:0.95ï¼‰", "")
            ir_tref  = st.text_input("åå°„æ¸©åº¦ T_ref[â„ƒ]ï¼ˆä¾‹:20ï¼‰", "")
            ir_tamb  = st.text_input("å¤–æ°—æ¸© T_amb[â„ƒ]ï¼ˆä¾‹:22ï¼‰", "")
            ir_rh    = st.text_input("ç›¸å¯¾æ¹¿åº¦ RH[%]ï¼ˆä¾‹:65ï¼‰", "")
            ir_dist  = st.text_input("æ’®å½±è·é›¢[m]ï¼ˆä¾‹:5ï¼‰", "")
            ir_ang   = st.text_input("æ’®å½±è§’åº¦[Â°]ï¼ˆä¾‹:10ï¼‰", "")
    st.markdown('</div>', unsafe_allow_html=True)

    vis_img = Image.open(vis_cam) if vis_cam is not None else (Image.open(vis_file) if vis_file is not None else None)
    ir_img  = Image.open(ir_file) if ir_file is not None else None

    # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    st.markdown('<div class="md-card">', unsafe_allow_html=True)
    p1, p2 = st.columns(2)
    with p1:
        if vis_img: st.image(vis_img, caption="å¯è¦–ç”»åƒ", use_container_width=True)
    with p2:
        if ir_img:  st.image(ir_img, caption="IRç”»åƒ", use_container_width=True)
    st.markdown('</div>', unsafe_allow_html=True)

    # 3) ä½ç½®
    st.markdown('<div class="md-card">', unsafe_allow_html=True)
    st.markdown('<div class="md-title">3) ä½ç½®æƒ…å ±ï¼ˆç¾åœ¨åœ° or æ‰‹å…¥åŠ›ï¼‰</div>', unsafe_allow_html=True)
    lat_val: Optional[float] = None
    lon_val: Optional[float] = None
    if HAVE_GEO:
        loc = st_geolocation(key="geoloc", label="ğŸ“ ç¾åœ¨åœ°ã‚’å–å¾—ï¼ˆãƒ–ãƒ©ã‚¦ã‚¶ã§è¨±å¯ï¼‰")
        if isinstance(loc, dict):
            lat_val = loc.get("latitude") or loc.get("lat")
            lon_val = loc.get("longitude") or loc.get("lon")
            try:
                if lat_val is not None: lat_val = float(lat_val)
                if lon_val is not None: lon_val = float(lon_val)
            except Exception:
                lat_val, lon_val = None, None
    else:
        st.info("ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®ãŸã‚ç°¡æ˜“æ–¹å¼ã§ã™ã€‚ä¸‹ã®ãƒœã‚¿ãƒ³æŠ¼ä¸‹â†’è¨±å¯ã§è‡ªå‹•å…¥åŠ›ã€‚")
        if st.button("ğŸ“ ç¾åœ¨åœ°ã‚’å–å¾—ï¼ˆç°¡æ˜“æ–¹å¼ï¼‰"):
            geolocate_fallback_via_query_params(show_widget=True)
        lat_val, lon_val = geolocate_fallback_via_query_params(show_widget=False)

    c1, c2, c3 = st.columns([1,1,2])
    with c1:
        lat_str = "" if lat_val is None else f"{lat_val:.6f}"
        lat_str = st.text_input("ç·¯åº¦", value=lat_str, key="lat_manual")
    with c2:
        lon_str = "" if lon_val is None else f"{lon_val:.6f}"
        lon_str = st.text_input("çµŒåº¦", value=lon_str, key="lon_manual")
    with c3:
        st.caption("â€» å–å¾—ã§ããªã„å ´åˆã¯æ‰‹å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    # åœ°å›³
    try:
        lat_f = float(lat_str) if lat_str else None
        lon_f = float(lon_str) if lon_str else None
    except Exception:
        lat_f, lon_f = None, None

    if lat_f is not None and lon_f is not None:
        m = folium.Map(location=[lat_f, lon_f], zoom_start=18, tiles="OpenStreetMap")
        folium.Marker([lat_f, lon_f], tooltip="å¯¾è±¡åœ°ç‚¹").add_to(m)
        st_folium(m, height=300, use_container_width=True)
    else:
        st.info("åœ°å›³è¡¨ç¤ºï¼šç·¯åº¦çµŒåº¦ãŒæœªæŒ‡å®šã§ã™ã€‚")

    st.markdown('</div>', unsafe_allow_html=True)

    # 4) å®Ÿè¡Œ
    st.markdown('<div class="md-card">', unsafe_allow_html=True)
    st.markdown('<div class="md-title">4) è§£æã®å®Ÿè¡Œ</div>', unsafe_allow_html=True)
    run = st.button("ğŸ” è©³ç´°åˆ†æï¼ˆRAG + ç”»åƒ + ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ï¼‰", use_container_width=True)
    st.markdown('</div>', unsafe_allow_html=True)

    if run:
        if not user_q:
            st.error("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            return

        # ç”»åƒæ‰€è¦‹ï¼ˆç°¡æ˜“ï¼‰
        vis_find = analyze_visual(vis_img) if vis_img else None
        ir_meta = {
            "has_ir": ir_img is not None,
            "emissivity": (locals().get('ir_emiss') or "ä¸æ˜").strip(),
            "t_ref": (locals().get('ir_tref') or "ä¸æ˜").strip(),
            "t_amb": (locals().get('ir_tamb') or "ä¸æ˜").strip(),
            "rh": (locals().get('ir_rh') or "ä¸æ˜").strip(),
            "dist": (locals().get('ir_dist') or "ä¸æ˜").strip(),
            "angle": (locals().get('ir_ang') or "ä¸æ˜").strip(),
        }
        ir_find = analyze_ir(ir_img, ir_meta) if ir_img else None

        # ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹æš«å®š
        rule_grade, rule_reason = rule_based_grade(vis_find, ir_find)
        rule_life = rule_based_life(rule_grade)

        # RAGæ¤œç´¢
        snippets = rag_search(user_q, have_ir=ir_img is not None, k=MAX_SNIPPETS)

        # ãƒ‰ãƒ¡ã‚¤ãƒ³ä¸€èˆ¬åŸå‰‡
        priors = domain_priors_text()

        # ç”»åƒ â†’ Gemini parts
        image_parts = []
        if vis_img: image_parts.append(image_to_inline_part(vis_img))
        if ir_img:  image_parts.append(image_to_inline_part(ir_img))

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = build_master_prompt(
            user_q=user_q,
            rag_snippets=snippets,
            priors=priors,
            vis_find=vis_find,
            ir_find=ir_find,
            rule_grade=rule_grade,
            rule_life=rule_life,
            ir_meta=ir_meta
        )

        # API
        try:
            api_key = st.secrets["gemini"]["API_KEY"]
        except (KeyError, FileNotFoundError):
            st.error("Gemini API Key ãŒæœªè¨­å®šã§ã™ã€‚.streamlit/secrets.toml ã« [gemini].API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
            return

        with st.spinner("Gemini 2.5 Flash ãŒåˆ†æä¸­â€¦"):
            try:
                result = call_gemini(api_key, prompt, image_parts)
                report_md = extract_text_from_gemini(result)
            except requests.HTTPError as e:
                st.error(f"APIã‚¨ãƒ©ãƒ¼: {e.response.status_code} {e.response.reason}\n{e.response.text[:500]}")
                return
            except Exception as e:
                st.error(f"å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
                return

        if not report_md:
            st.warning("ãƒ¬ãƒãƒ¼ãƒˆãŒç©ºã§ã—ãŸã€‚å…¥åŠ›ï¼ˆè³ªå•/ç”»åƒ/PDFï¼‰ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
            return

        # 5) çµæœè¡¨ç¤ºï¼ˆç·åˆè©•ä¾¡ â†’ è©³ç´°ï¼‰
        st.markdown('<div class="md-card good-shadow jp-report">', unsafe_allow_html=True)
        st.markdown('<div class="md-title">5) è§£æçµæœ</div>', unsafe_allow_html=True)
        st.markdown('</div>', unsafe_allow_html=True)

        # ç·åˆè©•ä¾¡ãƒ–ãƒ­ãƒƒã‚¯æŠ½å‡º
        summary_block = None
        try:
            pattern = r"(?:^|\n)##?\s*ç·åˆè©•ä¾¡[\s\S]*?(?=\n##?\s|\Z)"
            m = re.search(pattern, report_md, re.IGNORECASE)
            if m:
                summary_block = m.group(0)
        except Exception:
            summary_block = None

        if summary_block:
            st.markdown('<div class="md-card good-shadow jp-report">', unsafe_allow_html=True)
            st.markdown('<div class="md-title">ğŸ§­ ç·åˆè©•ä¾¡ï¼ˆå…ˆé ­è¦ç´„ï¼‰</div>', unsafe_allow_html=True)
            st.markdown(f"<div class='jp-report'>{summary_block}</div>", unsafe_allow_html=True)
            st.markdown('</div>', unsafe_allow_html=True)

        with st.expander("è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤º", expanded=(summary_block is None)):
            # å…ˆé ­ã«ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹æš«å®šï¼ˆå‚è€ƒï¼‰ã‚‚æ·»ãˆã¦å¯è¦–åŒ–
            st.markdown("**ï¼ˆå‚è€ƒï¼‰ç”»åƒï¼‹ä¸€èˆ¬åŸå‰‡ã«ã‚ˆã‚‹æš«å®šè©•ä¾¡**")
            st.markdown(f"- æš«å®šã‚°ãƒ¬ãƒ¼ãƒ‰: `{rule_grade}`ï¼ˆ{rule_reason}ï¼‰")
            st.markdown(f"- å‚è€ƒå¯¿å‘½: `{rule_life}`")
            st.markdown("---")
            st.markdown(f"<div class='jp-report'>{report_md}</div>", unsafe_allow_html=True)

        st.download_button(
            "ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            report_md,
            file_name="building_health_report.md",
            mime="text/markdown",
            use_container_width=True
        )

    st.caption("Â© å»ºç‰©è¨ºæ–­ãã‚“ â€” å¯è¦–/èµ¤å¤– Ã— RAG Ã— ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ Ã— Gemini 2.5 Flashã€‚")


if __name__ == "__main__":
    main()
