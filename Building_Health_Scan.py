# --- Python 3.12.x äº’æ›ã®è»½é‡å®Ÿè£…ï¼ˆé‡ä¾å­˜ãªã—ï¼‰ ---
# * ç”»åƒ: ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆå¯è¦–/èµ¤å¤–ï¼‰ï¼†ã‚«ãƒ¡ãƒ©æ’®å½±ï¼ˆå¯è¦–ï¼‰
# * PDF RAG: ç´”Pythonã®ç°¡æ˜“ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã§Top-KæŠ½å‡º
# * Gemini 2.0 Flash: ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ï¼ˆç”»åƒbase64ï¼‰ã§è©³ç´°åˆ†æ
# * ã‚¹ãƒãƒ›å‘ã‘UI: å¤§ãƒœã‚¿ãƒ³/è¦‹å‡ºã—/ç¸¦é…ç½®ã€çµæœã®ã¿è¡¨ç¤ºï¼‹DL

import base64
import io
import os
import re
import unicodedata
from typing import List, Tuple, Dict

import streamlit as st
import requests
import PyPDF2
from PIL import Image

# ----------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ----------------------------
def normalize_text(text: str) -> str:
    # å…¨è§’â†’åŠè§’ã€æ”¹è¡Œâ†’ç©ºç™½ã€é€£ç¶šç©ºç™½ã®åœ§ç¸®
    text = unicodedata.normalize("NFKC", text)
    text = text.replace("\r", " ").replace("\n", " ")
    text = re.sub(r"\s+", " ", text).strip()
    return text

def extract_text_from_pdf(pdf_path: str) -> str:
    if not os.path.exists(pdf_path):
        return ""
    text = ""
    try:
        with open(pdf_path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            for p in reader.pages:
                t = p.extract_text() or ""
                text += t + "\n"
    except Exception as e:
        text += f"\n[PDFèª­è¾¼ã‚¨ãƒ©ãƒ¼:{pdf_path}:{e}]"
    return normalize_text(text)

def chunk_text(text: str, max_chars: int = 900, overlap: int = 120) -> List[str]:
    # æ–‡å­—æ•°ãƒ™ãƒ¼ã‚¹ã§ç´ ç›´ã«åˆ†å‰²ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ä»˜ãï¼‰
    chunks = []
    i = 0
    N = len(text)
    while i < N:
        end = min(i + max_chars, N)
        chunk = text[i:end]
        chunk = chunk.strip()
        if chunk:
            chunks.append(chunk)
        i = end - overlap if end - overlap > i else end
    return chunks

def tokenize(s: str) -> List[str]:
    s = s.lower()
    # è¨˜å·é™¤å»ï¼†æ—¥æœ¬èªã‚‚å«ã‚ã¦ç°¡æ˜“ãƒˆãƒ¼ã‚¯ãƒ³åŒ–
    s = re.sub(r"[^a-z0-9ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³_]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s.split()

def simple_retrieval_score(query: str, chunk: str) -> float:
    # ã‚¯ã‚¨ãƒªèªã®ä¸€è‡´å€‹æ•°ï¼‹é »åº¦ã‚’ç´ æœ´ã«åŠ ç‚¹ï¼ˆè»½é‡ãƒ»éä¾å­˜ï¼‰
    q_tokens = set(tokenize(query))
    c_tokens = tokenize(chunk)
    if not q_tokens or not c_tokens:
        return 0.0
    score = 0.0
    for qt in q_tokens:
        score += c_tokens.count(qt) * 1.0
        if qt in c_tokens:
            score += 0.5
    # ã‚ãšã‹ã«é•·ã•ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆå†—é•·é˜²æ­¢ï¼‰
    score = score / (1.0 + len(chunk) / 2000.0)
    return score

def topk_chunks(query: str, corpus: List[Tuple[str, str]], k: int = 4) -> List[Tuple[str, str]]:
    # corpus: (source_name, chunk_text)
    scored = []
    for src, ch in corpus:
        scored.append((simple_retrieval_score(query, ch), src, ch))
    scored.sort(key=lambda x: x[0], reverse=True)
    out = []
    for s, src, ch in scored[:k]:
        if s <= 0:
            continue
        out.append((src, ch))
    # ã²ã¨ã¤ã‚‚å½“ãŸã‚‰ãªã„ã¨ãã¯æœ€åˆã®çŸ­ã„ã‚‚ã®ã‚’ä¿é™ºã§1ã¤
    if not out and corpus:
        out = [corpus[0]]
    return out

def image_to_inline_part(image: Image.Image, mime: str = "image/jpeg", max_width: int = 1400) -> Dict:
    # JPEGã«çµ±ä¸€ï¼ˆPNGã‚‚JPEGåŒ–ï¼‰ã€‚å¹…ã‚’æŠ‘ãˆã¦ã‚µã‚¤ã‚ºå‰Šæ¸›ã€‚
    if image.mode != "RGB":
        image = image.convert("RGB")
    if image.width > max_width:
        r = max_width / float(image.width)
        image = image.resize((max_width, int(image.height * r)))
    buf = io.BytesIO()
    image.save(buf, format="JPEG", quality=90, optimize=True)
    b64 = base64.b64encode(buf.getvalue()).decode("utf-8")
    return {"inline_data": {"mime_type": mime, "data": b64}}

# ----------------------------
# Gemini å‘¼ã³å‡ºã—
# ----------------------------
def call_gemini(api_key: str, prompt_text: str, image_parts: List[Dict]) -> Dict:
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}"
    headers = {"Content-Type": "application/json"}
    # contentsã¯é †åºãŒé‡è¦ï¼šã¾ãšæŒ‡ç¤ºãƒ†ã‚­ã‚¹ãƒˆã€ç¶šã„ã¦ç”»åƒãƒ‘ãƒ¼ãƒ„ã‚’æ¸¡ã™
    parts = [{"text": prompt_text}]
    parts.extend(image_parts)
    payload = {"contents": [{"parts": parts}]}
    resp = requests.post(url, headers=headers, json=payload, timeout=60)
    resp.raise_for_status()
    return resp.json()

def extract_text_from_gemini(result: Dict) -> str:
    try:
        return result["candidates"][0]["content"]["parts"][0]["text"]
    except Exception:
        return ""

# ----------------------------
# ãƒã‚¹ã‚¿ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆç°¡æ½”ãƒ»çµæœä¸»ç¾©ï¼‰
# ----------------------------
def build_master_prompt(user_q: str,
                        rag_snippets: List[Tuple[str, str]],
                        ir_meta: Dict) -> str:
    # RAGã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ•´å½¢ï¼ˆå‡ºæ‰€æ˜è¨˜ï¼‰
    rag_block = []
    for src, txt in rag_snippets:
        rag_block.append(f"[{src}] {txt}")
    rag_text = "\n".join(rag_block)

    ir_note = ""
    if ir_meta.get("has_ir"):
        ir_note = (
            "æ³¨: æ·»ä»˜ã«ã¯èµ¤å¤–ç·šç”»åƒãŒå«ã¾ã‚Œã¾ã™ã€‚æ¸©åº¦åˆ†å¸ƒãƒ»ç†±ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ»æ½œåœ¨çš„å«æ°´ã‚„æ–­ç†±ä¸è‰¯ç­‰ã®å¯èƒ½æ€§ã«ç•™æ„ã—ã€"
            "IRãƒ¡ã‚¿ï¼ˆÎµ/åå°„æ¸©åº¦/å¤–æ°—æ¸©/æ¹¿åº¦/è·é›¢/è§’åº¦/ãƒ¬ãƒ³ã‚¸ï¼‰ä¸è¶³æ™‚ã¯ä¿¡é ¼åº¦ã‚’ä¸‹ã’ã¦ã‚³ãƒ¡ãƒ³ãƒˆã—ã¦ãã ã•ã„ã€‚\n"
            f"IRãƒ¡ã‚¿: Îµ={ir_meta.get('emissivity','ä¸æ˜')}, T_ref={ir_meta.get('t_ref','ä¸æ˜')}â„ƒ, "
            f"T_amb={ir_meta.get('t_amb','ä¸æ˜')}â„ƒ, RH={ir_meta.get('rh','ä¸æ˜')}%, "
            f"è·é›¢={ir_meta.get('dist','ä¸æ˜')}m, è§’åº¦={ir_meta.get('angle','ä¸æ˜')}Â°.\n"
        )

    prompt = f"""
ã‚ãªãŸã¯éç ´å£Šæ¤œæŸ»ãƒ»å»ºç¯‰ãƒ»ææ–™å­¦ã®ä¸Šç´šè¨ºæ–­å£«ã€‚å›½åœŸäº¤é€šçœï¼ˆMLITï¼‰åŸºæº–ã«æ•´åˆã—ã€ä¸ãˆã‚‰ã‚ŒãŸæ ¹æ‹ ã®ã¿ã§ç°¡æ½”ã‹ã¤æ­£ç¢ºã«æ—¥æœ¬èªãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã›ã‚ˆã€‚
ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã¯ç¦æ­¢ã€‚æ•°å€¤é–¾å€¤ã‚„å®šç¾©ã¯RAGæŠœç²‹ã«ã‚ã‚‹å ´åˆã®ã¿ä½¿ç”¨ã€‚ç„¡ã„å ´åˆã¯ã€Œæœªæ²è¼‰ã€ã¨æ˜ç¤ºã™ã‚‹ã€‚

# å…¥åŠ›
- ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•: {user_q}
- RAGæŠœç²‹ï¼ˆå‡ºæ‰€ä»˜ãã€ã“ã‚Œä»¥å¤–ã¯æ ¹æ‹ ã«ã—ãªã„ï¼‰:
{rag_text}

{ir_note}
# å‡ºåŠ›ä»•æ§˜ï¼ˆâ€œçµæœã®ã¿â€ã®Markdownï¼‰
1. **ç·åˆè©•ä¾¡**  
   - ã‚°ãƒ¬ãƒ¼ãƒ‰ï¼ˆA/B/C/Dï¼‰ã¨ä¸»å› ï¼ˆ1â€“2è¡Œï¼‰
   - æ¨å®šæ®‹å­˜å¯¿å‘½ï¼ˆå¹…ã€ä¾‹ï¼š7â€“12å¹´ï¼‰ï¼ï¼ˆä»»æ„ï¼‰è£œä¿®å¾Œã®æœŸå¾…å¯¿å‘½
2. **ä¸»è¦æ ¹æ‹ ï¼ˆå¯è¦–/IR/NDTï¼‰**  # ç”»åƒã‹ã‚‰èª­ã¿å–ã‚Œã‚‹äº‹å®Ÿã«é™å®š
3. **MLITåŸºæº–ã¨ã®é–¢ä¿‚**  # RAGå†…ã®æ–‡æ›¸åï¼‹ç¯€/è¦‹å‡ºã—ã§ç°¡æ½”ã«
4. **æ¨å¥¨å¯¾å¿œï¼ˆå„ªå…ˆåº¦é †ï¼‰**
5. **é™ç•Œã¨è¿½åŠ ãƒ‡ãƒ¼ã‚¿è¦æœ›**  # ç²¾åº¦å‘ä¸Šã«å¿…è¦ãªè¿½è£œæƒ…å ±

æ³¨æ„: ç”»åƒã‹ã‚‰ç›´æ¥æ¸¬ã‚Œãªã„å€¤ï¼ˆã²ã³å¹…ç­‰ï¼‰ã¯æ–­å®šã—ãªã„ã€‚IRãƒ¡ã‚¿ä¸è¶³æ™‚ã¯ä¿¡é ¼åº¦ã‚’ä¸‹ã’ã‚‹æ—¨ã‚’æ˜è¨˜ã€‚
"""
    return normalize_text(prompt)

# ----------------------------
# RAGæº–å‚™ï¼ˆ3PDFã‚’çµ±åˆï¼‰
# ----------------------------
@st.cache_resource(show_spinner=False)
def load_rag_corpus() -> List[Tuple[str, str]]:
    sources = [
        ("Structure_Base.pdf", "Structure_Base.pdf"),
        ("ä¸Šå³¶ç”º å…¬å…±æ–½è¨­ç­‰ç·åˆç®¡ç†è¨ˆç”»", "kamijimachou_Public_facility_management_plan.pdf"),
        ("æ¸¯åŒº å…¬å…±æ–½è¨­ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆè¨ˆç”»", "minatoku_Public_facility_management_plan.pdf"),
    ]
    corpus: List[Tuple[str, str]] = []
    for name, path in sources:
        text = extract_text_from_pdf(path)
        chunks = chunk_text(text, max_chars=900, overlap=120)
        for ch in chunks:
            if len(ch) >= 40:
                corpus.append((name, ch))
    # ç©ºã®å ´åˆã‚‚è¿”ã™ï¼ˆå¾Œæ®µã§ä¿é™ºå‡¦ç†ï¼‰
    return corpus

# ----------------------------
# Streamlit ã‚¢ãƒ—ãƒª
# ----------------------------
def main():
    st.set_page_config(page_title="Building Health Scan (Gemini)", layout="wide")
    st.markdown("## Building Health Scan â€” ã‚¹ãƒãƒ›å¯¾å¿œï¼ˆå¯è¦–/èµ¤å¤– + Gemini è©³ç´°åˆ†æï¼‰")

    # å…¥åŠ›ï¼šãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•
    user_q = st.text_input("è³ªå•ï¼ˆä¾‹ï¼šå¤–å£ã‚¿ã‚¤ãƒ«ã®ã²ã³å‰²ã‚ŒåŸºæº–ã¨æ¨å®šå¯¿å‘½ï¼‰", "")

    # ç”»åƒå…¥åŠ›UIï¼ˆã‚¹ãƒãƒ›å‰æï¼šç¸¦ä¸¦ã³ãƒ»å¤§è¦‹å‡ºã—ï¼‰
    st.markdown("### ç”»åƒå…¥åŠ›")
    st.markdown("**å¯è¦–ç”»åƒï¼ˆã©ã¡ã‚‰ã‹1ã¤ï¼‰**")
    col1, col2 = st.columns(2)
    with col1:
        vis_file = st.file_uploader("å¯è¦–ç”»åƒã‚’é¸æŠï¼ˆJPG/PNGï¼‰", type=["jpg", "jpeg", "png"], key="vis_up")
    with col2:
        vis_cam = st.camera_input("ã‚«ãƒ¡ãƒ©ã§æ’®å½±ï¼ˆå¯è¦–ï¼‰", key="vis_cam")

    st.markdown("**èµ¤å¤–ç·šï¼ˆIRï¼‰ç”»åƒï¼ˆä»»æ„ãƒ»å¤–ä»˜ã‘ã‚«ãƒ¡ãƒ©ã®å†™çœŸã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼‰**")
    ir_file = st.file_uploader("IRç”»åƒã‚’é¸æŠï¼ˆJPG/PNGï¼‰", type=["jpg", "jpeg", "png"], key="ir_up")

    with st.expander("IRãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆä»»æ„ãƒ»ç²¾åº¦å‘ä¸Šï¼‰"):
        ir_emiss = st.text_input("æ”¾å°„ç‡ Îµï¼ˆä¾‹: 0.95ï¼‰", "")
        ir_tref  = st.text_input("åå°„æ¸©åº¦ T_ref [â„ƒ]ï¼ˆä¾‹: 20ï¼‰", "")
        ir_tamb  = st.text_input("å¤–æ°—æ¸© T_amb [â„ƒ]ï¼ˆä¾‹: 22ï¼‰", "")
        ir_rh    = st.text_input("ç›¸å¯¾æ¹¿åº¦ RH [%]ï¼ˆä¾‹: 65ï¼‰", "")
        ir_dist  = st.text_input("æ’®å½±è·é›¢ [m]ï¼ˆä¾‹: 5ï¼‰", "")
        ir_ang   = st.text_input("æ’®å½±è§’åº¦ [Â°]ï¼ˆä¾‹: 10ï¼‰", "")

    # è¡¨ç¤ºç”¨ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    vis_img = None
    if vis_cam is not None:
        vis_img = Image.open(vis_cam)
    elif vis_file is not None:
        vis_img = Image.open(vis_file)
    if vis_img is not None:
        st.image(vis_img, caption="å¯è¦–ç”»åƒ", use_container_width=True)

    ir_img = None
    if ir_file is not None:
        ir_img = Image.open(ir_file)
        st.image(ir_img, caption="IRç”»åƒ", use_container_width=True)

    # å®Ÿè¡Œãƒœã‚¿ãƒ³
    run = st.button("ğŸ” Geminiã§è©³ç´°åˆ†æï¼ˆçµæœã®ã¿è¡¨ç¤ºï¼‰", use_container_width=True)

    if run:
        if not user_q:
            st.error("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            return

        # RAGæº–å‚™
        corpus = load_rag_corpus()
        snippets = topk_chunks(user_q, corpus, k=4)

        # ç”»åƒâ†’Geminiãƒ‘ãƒ¼ãƒ„
        image_parts = []
        if vis_img is not None:
            image_parts.append(image_to_inline_part(vis_img, mime="image/jpeg"))
        if ir_img is not None:
            image_parts.append(image_to_inline_part(ir_img, mime="image/jpeg"))

        # IRãƒ¡ã‚¿æ•´å½¢
        ir_meta = {
            "has_ir": ir_img is not None,
            "emissivity": ir_emiss.strip() or "ä¸æ˜",
            "t_ref": ir_tref.strip() or "ä¸æ˜",
            "t_amb": ir_tamb.strip() or "ä¸æ˜",
            "rh": ir_rh.strip() or "ä¸æ˜",
            "dist": ir_dist.strip() or "ä¸æ˜",
            "angle": ir_ang.strip() or "ä¸æ˜",
        }

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        prompt = build_master_prompt(user_q, snippets, ir_meta)

        # Geminiå‘¼ã³å‡ºã—
        try:
            api_key = st.secrets["gemini"]["API_KEY"]
        except KeyError:
            st.error("Gemini API Key ãŒæœªè¨­å®šã§ã™ã€‚.streamlit/secrets.toml ã«è¨­å®šã—ã¦ãã ã•ã„ã€‚")
            return

        with st.spinner("GeminiãŒåˆ†æä¸­â€¦"):
            try:
                result = call_gemini(api_key, prompt, image_parts)
                report = extract_text_from_gemini(result)
            except requests.HTTPError as e:
                st.error(f"APIã‚¨ãƒ©ãƒ¼: {e.response.text[:500]}")
                return
            except Exception as e:
                st.error(f"å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
                return

        if not report:
            st.warning("å‡ºåŠ›ãŒç©ºã§ã—ãŸã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚„ç”»åƒ/è³ªå•å†…å®¹ã‚’è¦‹ç›´ã—ã¦ãã ã•ã„ã€‚")
            return

        # çµæœã®ã¿è¡¨ç¤º
        st.markdown("## è§£æçµæœ")
        st.markdown(report)
        st.download_button("ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", report, file_name="report.md", mime="text/markdown", use_container_width=True)

        # ç°¡æ˜“ãƒˆãƒ¬ãƒ¼ã‚¹ï¼ˆä»»æ„ã§è¡¨ç¤ºï¼‰
        with st.expander("ï¼ˆå‚è€ƒï¼‰ä½¿ç”¨ã—ãŸRAGæŠœç²‹", expanded=False):
            for src, ch in snippets:
                st.markdown(f"**{src}**ï¼š{ch[:500]}{'...' if len(ch)>500 else ''}")

if __name__ == "__main__":
    main()
