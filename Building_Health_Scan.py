import pkgutil
if not hasattr(pkgutil, "ImpImporter"):
    pkgutil.ImpImporter = pkgutil.zipimporter

import streamlit as st
import requests
import json
import faiss
import numpy as np
import PyPDF2
from PIL import Image
from sentence_transformers import SentenceTransformer
from transformers import BlipProcessor, BlipForConditionalGeneration

# --------------- è¨­å®š ----------------
st.set_page_config(
    page_title="ã‚¹ãƒãƒ›ã§å»ºç‰©åˆ†æRAG",
    layout="centered",  # ã‚¹ãƒãƒ›ã§ã¯centeredæ¨å¥¨
    initial_sidebar_state="collapsed"
)

# --------------- ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¬ã‚¤ãƒ‰ä»˜ãã‚¤ãƒ¡ãƒ¼ã‚¸å›³ ----------------
with st.expander("ğŸ“± ã‚¹ãƒãƒ›UIã‚¤ãƒ¡ãƒ¼ã‚¸å›³ï¼ˆã‚¯ãƒªãƒƒã‚¯ã§é–‹é–‰ï¼‰", expanded=True):
    st.image("https://raw.githubusercontent.com/streamlit/streamlit-example-apps/main/assets/mobile_mockup_simple.png", caption="ãƒ¢ãƒã‚¤ãƒ«åˆ©ç”¨ã‚¤ãƒ¡ãƒ¼ã‚¸å›³", use_column_width=True)

# --------------- å„ç¨®é–¢æ•° ----------------
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, "rb") as f:
        reader = PyPDF2.PdfReader(f)
        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

def split_text_into_chunks(text, max_length=500):
    return [text[i:i+max_length] for i in range(0, len(text), max_length)]

def create_vector_index(chunks, model):
    embeddings = model.encode(chunks, convert_to_numpy=True, batch_size=32)
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)
    index.add(embeddings)
    return index

@st.cache_resource(show_spinner=False)
def load_pdf_index():
    pdf_paths = [
        "Structure_Base.pdf",
        "kamijimachou_Public_facility_management_plan.pdf",
        "minatoku_Public_facility_management_plan.pdf"
    ]
    combined_text = ""
    for path in pdf_paths:
        combined_text += extract_text_from_pdf(path) + "\n"
    chunks = split_text_into_chunks(combined_text)
    chunks = [chunk for chunk in chunks if chunk.strip() and len(chunk.strip()) > 5]
    vec_model = SentenceTransformer('all-MiniLM-L6-v2')
    tokenizer = vec_model.tokenizer
    valid_chunks = [chunk for chunk in chunks if len(tokenizer.encode(chunk, add_special_tokens=True)) > 0]
    index = create_vector_index(valid_chunks, vec_model)
    return index, valid_chunks, vec_model

def search_relevant_chunks(query, model, index, chunks, top_k=3):
    query_vec = model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_vec, top_k)
    return [chunks[i] for i in indices[0] if i < len(chunks)]

@st.cache_resource(show_spinner=False)
def load_caption_model():
    processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
    model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
    return processor, model

def generate_image_caption(image, processor, model):
    max_width = 800
    if image.width > max_width:
        ratio = max_width / image.width
        new_size = (max_width, int(image.height * ratio))
        image = image.resize(new_size)
    if image.mode != 'RGB':
        image = image.convert('RGB')
    inputs = processor([image], return_tensors="pt", padding=True)
    out = model.generate(**inputs)
    caption = processor.decode(out[0], skip_special_tokens=True)
    return caption

def generate_report_with_gemini(prompt_text):
    try:
        api_key = st.secrets["gemini"]["API_KEY"]
    except KeyError:
        return {"error": "Gemini API KeyãŒ .streamlit/secrets.toml ã«è¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚"}
    endpoint = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}"
    headers = {"Content-Type": "application/json"}
    payload = {"contents": [{"parts": [{"text": prompt_text}]}]}
    try:
        response = requests.post(endpoint, headers=headers, json=payload)
        response.raise_for_status()
        return response.json()
    except Exception as e:
        return {"error": str(e)}

# ---------------- UIãƒ¡ã‚¤ãƒ³ ------------------
st.title("ã‚¹ãƒãƒ›ç‰¹åŒ–ï¼šãƒ•ã‚¡ã‚·ãƒªãƒ†ã‚£åˆ†æRAG")
st.markdown(
    """
    1. **å»ºç‰©ã‚„å¤–å£ã®ç”»åƒ**ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»æ’®å½±  
    2. **è³ªå•ã‚’å…¥åŠ›**  
    3. **ã€Œãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã€ãƒœã‚¿ãƒ³ã‚’ã‚¿ãƒƒãƒ—**  
    ---  
    ç”»åƒï¼‹å°‚é–€è³‡æ–™ã‚’æ´»ç”¨ã—ãŸåˆ†æAIãŒã€**éç ´å£Šæ¤œæŸ»ãƒ»å»ºç¯‰åˆ†é‡ã®å°‚é–€çŸ¥è­˜**ã§å›ç­”ã—ã¾ã™ï¼
    """
)

with st.form(key="analysis_form"):
    # ã‚¹ãƒãƒ›ã§ã¯ã‚«ãƒ¡ãƒ©å…¥åŠ›ã‚’æ¨å¥¨ã—ã€ä¸€åº¦ã«ä¸€ã¤ã®ç”»åƒ
    st.markdown("### 1. ç”»åƒã®å…¥åŠ›")
    upload_col, camera_col = st.columns(2)
    image = None
    with upload_col:
        image_file = st.file_uploader("ç”»åƒã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["jpg", "jpeg", "png"], key="up_file")
        if image_file:
            image = Image.open(image_file)
    with camera_col:
        image_camera = st.camera_input("æ’®å½±", key="camera_in")
        if image_camera:
            image = Image.open(image_camera)
    if image:
        st.image(image, caption="é¸æŠä¸­ã®ç”»åƒ", use_column_width=True)
        processor, cap_model = load_caption_model()
        with st.spinner("ç”»åƒè§£æä¸­..."):
            try:
                caption = generate_image_caption(image, processor, cap_model)
            except Exception as e:
                caption = f"ã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}"
        st.info(f"ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³ï¼š{caption}")
    else:
        caption = ""
        st.warning("ç”»åƒã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚")

    st.markdown("### 2. è³ªå•ã‚’å…¥åŠ›")
    user_query = st.text_area("è³ªå•ã‚’å…¥åŠ›ï¼ˆä¾‹ï¼šå¤–å£ã®ã²ã³å‰²ã‚Œã®åŸºæº–ã¯ï¼Ÿï¼‰", max_chars=150)

    st.form_submit_button("ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ", use_container_width=True)

if image and user_query:
    # ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    with st.spinner("PDFã¨ç”»åƒæƒ…å ±ã‚’çµ±åˆã—ã¦AIãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆä¸­..."):
        index, chunks, vec_model = load_pdf_index()
        relevant_chunks = search_relevant_chunks(user_query, vec_model, index, chunks)
        context = "\n\n".join(relevant_chunks)
        prompt = (
            f"ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•: {user_query}\n\n"
            f"ä»¥ä¸‹ã¯PDFï¼ˆStructure_Base.pdfç­‰ï¼‰ã‹ã‚‰ã®é–¢é€£æƒ…å ±:\n{context}\n\n"
        )
        if caption:
            prompt += f"ç”»åƒã‚­ãƒ£ãƒ—ã‚·ãƒ§ãƒ³:\n{caption}\n\n"
        prompt += (
            "ã“ã‚Œã‚‰ã‚’å…ƒã«ã€éç ´å£Šæ¤œæŸ»ãƒ»å»ºç¯‰ãƒ»ææ–™å­¦ã®çŸ¥è­˜ã‚’æ´»ç”¨ã—ã€å›½åœŸäº¤é€šçœåŸºæº–ã§åˆ†æãƒ¬ãƒãƒ¼ãƒˆã‚’æ—¥æœ¬èªã§ä½œæˆã›ã‚ˆã€‚"
            " åŠ£åŒ–åº¦Aï½Dãƒ»æ¨å®šå¯¿å‘½ã‚‚æ˜ç¤ºã€‚èª¤æƒ…å ±ã¯æ’é™¤ã€‚"
        )
        result = generate_report_with_gemini(prompt)
        if "error" in result:
            st.error(f"APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {result['error']}")
        else:
            try:
                report_text = result["candidates"][0]["content"]["parts"][0]["text"]
            except Exception as e:
                st.error("ãƒ¬ãƒãƒ¼ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: " + str(e))
                report_text = None
            if report_text:
                st.markdown("## ç”Ÿæˆãƒ¬ãƒãƒ¼ãƒˆ")
                st.markdown(report_text)
                st.download_button("ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", report_text, file_name="report.txt", mime="text/plain")
else:
    st.info("ç”»åƒã¨è³ªå•ã®ä¸¡æ–¹ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

st.markdown("---")
st.caption("ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³åˆ©ç”¨æ™‚ã¯ç”»é¢ã‚’ç¸¦ã«ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€å„é …ç›®ã‚’é †ç•ªã«å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
