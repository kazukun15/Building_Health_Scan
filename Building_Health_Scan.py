# -*- coding: utf-8 -*-
# ===========================================================
# å»ºç‰©è¨ºæ–­ãã‚“ï¼ˆã‚¹ãƒãƒ›å¯¾å¿œãƒ»ãƒãƒ†ãƒªã‚¢ãƒ«ãƒ‡ã‚¶ã‚¤ãƒ³é¢¨UIãƒ»æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆé©ç”¨ï¼‰
# - å¯è¦–/èµ¤å¤– ç”»åƒï¼šã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ + ã‚«ãƒ¡ãƒ©
# - EXIFã‹ã‚‰GPSæŠ½å‡º â†’ Foliumåœ°å›³ã«è¡¨ç¤ºï¼ˆæ‰‹å…¥åŠ›ã‚‚å¯ï¼‰
# - 3PDFã‚’RAGï¼ˆè»½é‡ã‚¹ã‚³ã‚¢ï¼‰â†’ Gemini 2.0 Flash ã§è©³ç´°åˆ†æ
# - çµæœã®ã¿è¡¨ç¤ºï¼šç·åˆè©•ä¾¡ã‚«ãƒ¼ãƒ‰ã‚’å…ˆé ­ã«ã€è©³ç´°ã¯å±•é–‹
# - ãƒ¬ãƒãƒ¼ãƒˆDLï¼ˆå…±æœ‰ï¼‰
# - ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤ºã« Noto Sans JP/Noto Serif JP ã‚’é©ç”¨
# Python 3.12 äº’æ›ï¼é‡ä¾å­˜ãªã—ï¼ˆè»½é‡ï¼‰
# ===========================================================

# Python 3.12: pkgutil.ImpImporterå‰Šé™¤å›é¿ï¼ˆå¤ã„ä¾å­˜ã®ãŸã‚ã®ä¿é™ºï¼‰
import pkgutil
if not hasattr(pkgutil, "ImpImporter"):
    pkgutil.ImpImporter = pkgutil.zipimporter

import os
import io
import re
import base64
import unicodedata
from typing import List, Tuple, Dict, Optional

import streamlit as st
import requests
import PyPDF2
from PIL import Image

# ä½ç½®æƒ…å ±ãƒ»åœ°å›³
from exif import Image as ExifImage   # pip install exif
import folium                         # pip install folium
from streamlit_folium import st_folium  # pip install streamlit-folium

# -----------------------------------------------------------
# å®šæ•°
# -----------------------------------------------------------
APP_TITLE = "å»ºç‰©è¨ºæ–­ãã‚“"
PDF_SOURCES = [
    ("Structure_Base.pdf", "Structure_Base.pdf"),
    ("ä¸Šå³¶ç”º å…¬å…±æ–½è¨­ç­‰ç·åˆç®¡ç†è¨ˆç”»", "kamijimachou_Public_facility_management_plan.pdf"),
    ("æ¸¯åŒº å…¬å…±æ–½è¨­ãƒãƒã‚¸ãƒ¡ãƒ³ãƒˆè¨ˆç”»", "minatoku_Public_facility_management_plan.pdf"),
]

# -----------------------------------------------------------
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# -----------------------------------------------------------
def normalize_text(text: str) -> str:
    text = unicodedata.normalize("NFKC", text)
    text = text.replace("\r", " ").replace("\n", " ")
    text = re.sub(r"\s+", " ", text).strip()
    return text

def extract_text_from_pdf(pdf_path: str) -> str:
    if not os.path.exists(pdf_path):
        return ""
    text = ""
    try:
        with open(pdf_path, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            for p in reader.pages:
                t = p.extract_text() or ""
                text += t + "\n"
    except Exception as e:
        text += f"\n[PDFèª­è¾¼ã‚¨ãƒ©ãƒ¼:{pdf_path}:{e}]"
    return normalize_text(text)

def chunk_text(text: str, max_chars: int = 900, overlap: int = 120) -> List[str]:
    chunks, i, N = [], 0, len(text)
    while i < N:
        end = min(i + max_chars, N)
        ch = text[i:end].strip()
        if ch:
            chunks.append(ch)
        i = end - overlap if end - overlap > i else end
    return chunks

def tokenize(s: str) -> List[str]:
    s = s.lower()
    s = re.sub(r"[^a-z0-9ä¸€-é¾¥ã-ã‚“ã‚¡-ãƒ³_]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s.split()

def simple_retrieval_score(query: str, chunk: str) -> float:
    q_tokens = set(tokenize(query))
    c_tokens = tokenize(chunk)
    if not q_tokens or not c_tokens:
        return 0.0
    score = 0.0
    for qt in q_tokens:
        score += c_tokens.count(qt) * 1.0
        if qt in c_tokens:
            score += 0.5
    # å†—é•·ãƒšãƒŠãƒ«ãƒ†ã‚£ï¼ˆé•·ã™ãã‚‹å¡Šã‚’å¼±ã‚ã‚‹ï¼‰
    score = score / (1.0 + len(chunk) / 2000.0)
    return score

def topk_chunks(query: str, corpus: List[Tuple[str, str]], k: int = 4) -> List[Tuple[str, str]]:
    scored = [(simple_retrieval_score(query, ch), src, ch) for (src, ch) in corpus]
    scored.sort(key=lambda x: x[0], reverse=True)
    out = [(src, ch) for (s, src, ch) in scored[:k] if s > 0]
    if not out and corpus:
        out = [corpus[0]]  # ä¿é™º
    return out

def image_to_inline_part(image: Image.Image, max_width: int = 1400) -> Dict:
    # Geminiã«Base64ã§é€ã‚‹ï¼ˆJPEGå›ºå®šï¼‰
    if image.mode != "RGB":
        image = image.convert("RGB")
    if image.width > max_width:
        r = max_width / float(image.width)
        image = image.resize((max_width, int(image.height * r)))
    buf = io.BytesIO()
    image.save(buf, format="JPEG", quality=90, optimize=True)
    b64 = base64.b64encode(buf.getvalue()).decode("utf-8")
    return {"inline_data": {"mime_type": "image/jpeg", "data": b64}}

# -----------------------------------------------------------
# EXIF â†’ GPS æŠ½å‡ºï¼ˆç·¯åº¦çµŒåº¦ã‚’10é€²åº¦ã«ï¼‰
# -----------------------------------------------------------
def _to_deg(value) -> float:
    # EXIFã®åº¦åˆ†ç§’[(num,den)...]â†’ 10é€²åº¦ã«å¤‰æ›
    try:
        d = float(value[0].numerator) / float(value[0].denominator)
        m = float(value[1].numerator) / float(value[1].denominator)
        s = float(value[2].numerator) / float(value[2].denominator)
        return d + (m / 60.0) + (s / 3600.0)
    except Exception:
        try:
            d, m, s = value
            return float(d) + float(m) / 60.0 + float(s) / 3600.0
        except Exception:
            return None

def extract_gps_from_image(uploaded_bytes: bytes) -> Optional[Tuple[float, float]]:
    try:
        exif = ExifImage(uploaded_bytes)
    except Exception:
        return None
    if not exif.has_exif:
        return None
    if not (hasattr(exif, "gps_latitude") and hasattr(exif, "gps_longitude")
            and hasattr(exif, "gps_latitude_ref") and hasattr(exif, "gps_longitude_ref")):
        return None
    lat = _to_deg(exif.gps_latitude)
    lon = _to_deg(exif.gps_longitude)
    if lat is None or lon is None:
        return None
    if exif.gps_latitude_ref in ["S", "s"]:
        lat = -lat
    if exif.gps_longitude_ref in ["W", "w"]:
        lon = -lon
    return (lat, lon)

# -----------------------------------------------------------
# Gemini å‘¼ã³å‡ºã—
# -----------------------------------------------------------
def call_gemini(api_key: str, prompt_text: str, image_parts: List[Dict]) -> Dict:
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}"
    headers = {"Content-Type": "application/json"}
    parts = [{"text": prompt_text}]
    parts.extend(image_parts)
    payload = {"contents": [{"parts": parts}]}
    resp = requests.post(url, headers=headers, json=payload, timeout=90)
    resp.raise_for_status()
    return resp.json()

def extract_text_from_gemini(result: Dict) -> str:
    try:
        return result["candidates"][0]["content"]["parts"][0]["text"]
    except Exception:
        return ""

# -----------------------------------------------------------
# ãƒã‚¹ã‚¿ãƒ¼ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆâ€œçµæœã®ã¿â€ãƒ»ç·åˆè©•ä¾¡å…ˆé ­ï¼‰
# -----------------------------------------------------------
def build_master_prompt(user_q: str,
                        rag_snippets: List[Tuple[str, str]],
                        ir_meta: Dict) -> str:
    rag_block = [f"[{src}] {txt}" for (src, txt) in rag_snippets]
    rag_text = "\n".join(rag_block)

    ir_note = ""
    if ir_meta.get("has_ir"):
        ir_note = (
            "æ³¨: æ·»ä»˜ã«ã¯èµ¤å¤–ç·šç”»åƒãŒå«ã¾ã‚Œã¾ã™ã€‚æ¸©åº¦åˆ†å¸ƒãƒ»ç†±ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒ»æ½œåœ¨çš„å«æ°´ã‚„æ–­ç†±ä¸è‰¯ç­‰ã®å¯èƒ½æ€§ã«ç•™æ„ã—ã€"
            "IRãƒ¡ã‚¿ï¼ˆÎµ/åå°„æ¸©åº¦/å¤–æ°—æ¸©/æ¹¿åº¦/è·é›¢/è§’åº¦/ãƒ¬ãƒ³ã‚¸ï¼‰ä¸è¶³æ™‚ã¯ä¿¡é ¼åº¦ã‚’ä¸‹ã’ã¦ã‚³ãƒ¡ãƒ³ãƒˆã—ã¦ãã ã•ã„ã€‚\n"
            f"IRãƒ¡ã‚¿: Îµ={ir_meta.get('emissivity','ä¸æ˜')}, T_ref={ir_meta.get('t_ref','ä¸æ˜')}â„ƒ, "
            f"T_amb={ir_meta.get('t_amb','ä¸æ˜')}â„ƒ, RH={ir_meta.get('rh','ä¸æ˜')}%, "
            f"è·é›¢={ir_meta.get('dist','ä¸æ˜')}m, è§’åº¦={ir_meta.get('angle','ä¸æ˜')}Â°.\n"
        )

    prompt = f"""
ã‚ãªãŸã¯éç ´å£Šæ¤œæŸ»ãƒ»å»ºç¯‰ãƒ»ææ–™å­¦ã®ä¸Šç´šè¨ºæ–­å£«ã€‚å›½åœŸäº¤é€šçœï¼ˆMLITï¼‰åŸºæº–ã«æ•´åˆã—ã€ä¸ãˆã‚‰ã‚ŒãŸæ ¹æ‹ ã®ã¿ã§ç°¡æ½”ã‹ã¤æ­£ç¢ºã«æ—¥æœ¬èªãƒ¬ãƒãƒ¼ãƒˆã‚’ä½œæˆã›ã‚ˆã€‚
ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³ã¯ç¦æ­¢ã€‚æ•°å€¤é–¾å€¤ã‚„å®šç¾©ã¯RAGæŠœç²‹ã«ã‚ã‚‹å ´åˆã®ã¿ä½¿ç”¨ã€‚ç„¡ã„å ´åˆã¯ã€Œæœªæ²è¼‰ã€ã¨æ˜ç¤ºã™ã‚‹ã€‚

# å…¥åŠ›
- ãƒ¦ãƒ¼ã‚¶ãƒ¼è³ªå•: {user_q}
- RAGæŠœç²‹ï¼ˆå‡ºæ‰€ä»˜ãã€ã“ã‚Œä»¥å¤–ã¯æ ¹æ‹ ã«ã—ãªã„ï¼‰:
{rag_text}

{ir_note}
# å‡ºåŠ›ä»•æ§˜ï¼ˆâ€œçµæœã®ã¿â€ã®Markdownï¼‰
1. **ç·åˆè©•ä¾¡**  
   - ã‚°ãƒ¬ãƒ¼ãƒ‰ï¼ˆA/B/C/Dï¼‰ã¨ä¸»å› ï¼ˆ1â€“2è¡Œï¼‰
   - æ¨å®šæ®‹å­˜å¯¿å‘½ï¼ˆå¹…ã€ä¾‹ï¼š7â€“12å¹´ï¼‰ï¼ï¼ˆä»»æ„ï¼‰è£œä¿®å¾Œã®æœŸå¾…å¯¿å‘½
2. **ä¸»è¦æ ¹æ‹ ï¼ˆå¯è¦–/IR/NDTï¼‰**  # ç”»åƒã‹ã‚‰èª­ã¿å–ã‚Œã‚‹äº‹å®Ÿã«é™å®š
3. **MLITåŸºæº–ã¨ã®é–¢ä¿‚**  # RAGå†…ã®æ–‡æ›¸åï¼‹ç¯€/è¦‹å‡ºã—ã§ç°¡æ½”ã«
4. **æ¨å¥¨å¯¾å¿œï¼ˆå„ªå…ˆåº¦é †ï¼‰**
5. **é™ç•Œã¨è¿½åŠ ãƒ‡ãƒ¼ã‚¿è¦æœ›**  # ç²¾åº¦å‘ä¸Šã«å¿…è¦ãªè¿½è£œæƒ…å ±

æ³¨æ„: ç”»åƒã‹ã‚‰ç›´æ¥æ¸¬ã‚Œãªã„å€¤ï¼ˆã²ã³å¹…ç­‰ï¼‰ã¯æ–­å®šã—ãªã„ã€‚IRãƒ¡ã‚¿ä¸è¶³æ™‚ã¯ä¿¡é ¼åº¦ã‚’ä¸‹ã’ã‚‹æ—¨ã‚’æ˜è¨˜ã€‚
""".strip()
    return normalize_text(prompt)

# -----------------------------------------------------------
# RAGï¼ˆ3PDFã‚’çµ±åˆã—ã¦è»½é‡Top-KæŠ½å‡ºï¼‰
# -----------------------------------------------------------
@st.cache_resource(show_spinner=False)
def load_rag_corpus() -> List[Tuple[str, str]]:
    corpus: List[Tuple[str, str]] = []
    for name, path in PDF_SOURCES:
        text = extract_text_from_pdf(path)
        chunks = chunk_text(text, max_chars=900, overlap=120)
        for ch in chunks:
            if len(ch) >= 40:
                corpus.append((name, ch))
    return corpus

# -----------------------------------------------------------
# UI: ãƒãƒ†ãƒªã‚¢ãƒ«ãƒ‡ã‚¶ã‚¤ãƒ³é¢¨ + æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆé©ç”¨CSS
# -----------------------------------------------------------
def inject_material_css():
    st.markdown("""
    <!-- Google Fonts: Noto Sans JP / Noto Serif JP -->
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans+JP:wght@400;500;700&family=Noto+Serif+JP:wght@500;700&display=swap" rel="stylesheet">
    <style>
    :root{
      --mdc-primary:#2962ff;
      --mdc-primary-variant:#0039cb;
      --mdc-secondary:#00b8d4;
      --mdc-bg:#f7f9fc;
      --mdc-surface:#ffffff;
      --mdc-on-primary:#ffffff;
      --radius:16px;
      --shadow:0 6px 18px rgba(0,0,0,.08);
    }
    .block-container{padding-top:1rem;padding-bottom:2rem;}
    body{background:var(--mdc-bg);}
    /* æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚¯ãƒ©ã‚¹ */
    .jp-sans{
      font-family:'Noto Sans JP', -apple-system, BlinkMacSystemFont, 'Segoe UI',
        'Hiragino Kaku Gothic ProN','Hiragino Sans','Meiryo', sans-serif !important;
      line-height:1.7;
      letter-spacing:0.01em;
    }
    .jp-serif{
      font-family:'Noto Serif JP','Hiragino Mincho ProN','Yu Mincho',serif !important;
      line-height:1.8;
      letter-spacing:0.01em;
    }
    .jp-report h1,.jp-report h2,.jp-report h3,
    .jp-report p,.jp-report li,.jp-report table{
      font-family:'Noto Sans JP', -apple-system, BlinkMacSystemFont, 'Segoe UI',
        'Hiragino Kaku Gothic ProN','Hiragino Sans','Meiryo', sans-serif !important;
    }
    .md-card{
      background:var(--mdc-surface);
      border-radius:var(--radius);
      box-shadow:var(--shadow);
      padding:1rem 1.1rem;
      margin-bottom:1rem;
      border:1px solid rgba(0,0,0,.04);
    }
    .md-title{font-size:1.1rem;font-weight:700;margin:0.2rem 0 0.6rem 0;}
    .md-sub{opacity:.85;font-size:.95rem;}
    .eval-chip{
      display:inline-block;padding:.35rem .7rem;border-radius:999px;
      color:#fff;font-weight:700;letter-spacing:.02em;
      background:linear-gradient(135deg, var(--mdc-primary), var(--mdc-secondary));
    }
    .primary-btn button{
      background:linear-gradient(135deg, var(--mdc-primary), var(--mdc-secondary)) !important;
      color:#fff !important;border:none !important;
      box-shadow:var(--shadow);
    }
    .good-shadow{box-shadow:var(--shadow);}
    </style>
    """, unsafe_allow_html=True)

# -----------------------------------------------------------
# ãƒ¡ã‚¤ãƒ³
# -----------------------------------------------------------
def main():
    st.set_page_config(page_title=APP_TITLE, layout="wide")
    inject_material_css()

    st.markdown(f"### ğŸ—ï¸ {APP_TITLE}")
    st.caption("ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³æœ€é©åŒ– / ç”»åƒï¼ˆå¯è¦–ãƒ»èµ¤å¤–ï¼‰ï¼‹RAGï¼‹Gemini 2.0 Flash ã§è©³ç´°åˆ†æ")

    # --- å…¥åŠ›è¡Œï¼ˆã‚¹ãƒãƒ›å‘ã‘ã«ç¸¦æ§‹æˆã€ã‚«ãƒ©ãƒ ã¯è‡ªå‹•ã§ç¸¦ç©ã¿ã«ãªã‚‹ï¼‰ ---
    st.markdown("#### 1) è³ªå•")
    user_q = st.text_input("ä¾‹ï¼šå¤–å£ã‚¿ã‚¤ãƒ«ã®ã²ã³å‰²ã‚ŒåŸºæº–ã¨æ¨å®šå¯¿å‘½", "", placeholder="åˆ†æã—ãŸã„ãƒ†ãƒ¼ãƒãƒ»è³ªå•ã‚’å…¥åŠ›")

    st.markdown("#### 2) ç”»åƒå…¥åŠ›ï¼ˆå¯è¦–/èµ¤å¤–ï¼‰")
    colA, colB = st.columns(2)
    with colA:
        st.markdown("**å¯è¦–ç”»åƒï¼ˆã©ã¡ã‚‰ã‹1ã¤ï¼‰**")
        vis_file = st.file_uploader("å¯è¦–ç”»åƒã‚’é¸æŠï¼ˆJPG/PNGï¼‰", type=["jpg", "jpeg", "png"], key="vis_up")
        vis_cam = st.camera_input("ã‚«ãƒ¡ãƒ©ã§æ’®å½±ï¼ˆå¯è¦–ï¼‰", key="vis_cam")
    with colB:
        st.markdown("**èµ¤å¤–ç·šï¼ˆIRï¼‰ç”»åƒï¼ˆä»»æ„ï¼šå¤–ä»˜ã‘ã‚«ãƒ¡ãƒ©å†™çœŸï¼‰**")
        ir_file = st.file_uploader("IRç”»åƒã‚’é¸æŠï¼ˆJPG/PNGï¼‰", type=["jpg", "jpeg", "png"], key="ir_up")

        with st.expander("IRãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ï¼ˆä»»æ„ãƒ»ç²¾åº¦å‘ä¸Šï¼‰"):
            ir_emiss = st.text_input("æ”¾å°„ç‡ Îµï¼ˆä¾‹: 0.95ï¼‰", "")
            ir_tref  = st.text_input("åå°„æ¸©åº¦ T_ref [â„ƒ]ï¼ˆä¾‹: 20ï¼‰", "")
            ir_tamb  = st.text_input("å¤–æ°—æ¸© T_amb [â„ƒ]ï¼ˆä¾‹: 22ï¼‰", "")
            ir_rh    = st.text_input("ç›¸å¯¾æ¹¿åº¦ RH [%]ï¼ˆä¾‹: 65ï¼‰", "")
            ir_dist  = st.text_input("æ’®å½±è·é›¢ [m]ï¼ˆä¾‹: 5ï¼‰", "")
            ir_ang   = st.text_input("æ’®å½±è§’åº¦ [Â°]ï¼ˆä¾‹: 10ï¼‰", "")

    # ç”»åƒãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
    vis_img = None
    vis_src_bytes = None
    if vis_cam is not None:
        vis_img = Image.open(vis_cam)
        vis_src_bytes = vis_cam.getvalue()
    elif vis_file is not None:
        vis_img = Image.open(vis_file)
        vis_src_bytes = vis_file.getvalue()

    ir_img = None
    ir_src_bytes = None
    if ir_file is not None:
        ir_img = Image.open(ir_file)
        ir_src_bytes = ir_file.getvalue()

    # ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    pv_col1, pv_col2 = st.columns(2)
    with pv_col1:
        if vis_img is not None:
            st.image(vis_img, caption="å¯è¦–ç”»åƒ", use_container_width=True)
    with pv_col2:
        if ir_img is not None:
            st.image(ir_img, caption="IRç”»åƒ", use_container_width=True)

    # --- ä½ç½®æƒ…å ±ï¼ˆEXIFâ†’GPS or æ‰‹å…¥åŠ›ï¼‰ ---
    st.markdown("#### 3) ä½ç½®æƒ…å ±ï¼ˆè‡ªå‹• or æ‰‹å…¥åŠ›ï¼‰")
    lat, lon = None, None
    if vis_src_bytes:
        gps = extract_gps_from_image(vis_src_bytes)
        if gps:
            lat, lon = gps
    if lat is None and ir_src_bytes:
        gps = extract_gps_from_image(ir_src_bytes)
        if gps:
            lat, lon = gps

    c1, c2, c3 = st.columns([1,1,2])
    with c1:
        lat = st.text_input("ç·¯åº¦ï¼ˆä¾‹ï¼š35.6804ï¼‰", value="" if lat is None else f"{lat:.6f}")
    with c2:
        lon = st.text_input("çµŒåº¦ï¼ˆä¾‹ï¼š139.7690ï¼‰", value="" if lon is None else f"{lon:.6f}")
    with c3:
        st.caption("â€» ç”»åƒEXIFã«ä½ç½®æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã‚Œã°è‡ªå‹•è¡¨ç¤ºã€‚ç„¡ã„å ´åˆã¯æ‰‹å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    # åœ°å›³è¡¨ç¤º
    map_row = st.container()
    with map_row:
        try:
            lat_f = float(lat) if lat else None
            lon_f = float(lon) if lon else None
        except Exception:
            lat_f, lon_f = None, None

        if lat_f is not None and lon_f is not None:
            m = folium.Map(location=[lat_f, lon_f], zoom_start=18, tiles="OpenStreetMap")
            folium.Marker([lat_f, lon_f], tooltip="å¯¾è±¡åœ°ç‚¹").add_to(m)
            st_folium(m, height=300, use_container_width=True)
        else:
            st.info("åœ°å›³è¡¨ç¤ºï¼šç·¯åº¦çµŒåº¦ãŒæœªæŒ‡å®šã§ã™ã€‚EXIFã«ä½ç½®ãŒç„¡ã„å ´åˆã¯æ‰‹å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    # --- å®Ÿè¡Œãƒœã‚¿ãƒ³ ---
    st.markdown("#### 4) è§£æã®å®Ÿè¡Œ")
    run = st.button("ğŸ” Geminiã§è©³ç´°åˆ†æï¼ˆçµæœã®ã¿è¡¨ç¤ºï¼‰", use_container_width=True)

    # -------------------------------------------------------
    # è§£æãƒ•ãƒ­ãƒ¼
    # -------------------------------------------------------
    if run:
        if not user_q:
            st.error("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
            return

        # RAGãƒ­ãƒ¼ãƒ‰ï¼†Top-KæŠ½å‡º
        corpus = load_rag_corpus()
        snippets = topk_chunks(user_q, corpus, k=4)

        # ç”»åƒ â†’ Gemini parts
        image_parts = []
        if vis_img is not None:
            image_parts.append(image_to_inline_part(vis_img))
        if ir_img is not None:
            image_parts.append(image_to_inline_part(ir_img))

        # IRãƒ¡ã‚¿
        ir_meta = {
            "has_ir": ir_img is not None,
            "emissivity": (ir_emiss or "ä¸æ˜").strip(),
            "t_ref": (ir_tref or "ä¸æ˜").strip(),
            "t_amb": (ir_tamb or "ä¸æ˜").strip(),
            "rh": (ir_rh or "ä¸æ˜").strip(),
            "dist": (ir_dist or "ä¸æ˜").strip(),
            "angle": (ir_ang or "ä¸æ˜").strip(),
        }

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        prompt = build_master_prompt(user_q, snippets, ir_meta)

        # Gemini API Key
        try:
            api_key = st.secrets["gemini"]["API_KEY"]
        except KeyError:
            st.error("Gemini API Key ãŒæœªè¨­å®šã§ã™ã€‚.streamlit/secrets.toml ã« [gemini].API_KEY ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚")
            return

        with st.spinner("GeminiãŒåˆ†æä¸­â€¦"):
            try:
                result = call_gemini(api_key, prompt, image_parts)
                report_md = extract_text_from_gemini(result)
            except requests.HTTPError as e:
                st.error(f"APIã‚¨ãƒ©ãƒ¼: {e.response.text[:500]}")
                return
            except Exception as e:
                st.error(f"å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
                return

        if not report_md:
            st.warning("ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›ãŒç©ºã§ã—ãŸã€‚å…¥åŠ›å†…å®¹ï¼ˆè³ªå•ãƒ»ç”»åƒãƒ»PDFï¼‰ã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
            return

        # ---- ãƒ¬ãƒãƒ¼ãƒˆï¼ˆæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆé©ç”¨ï¼‰ ----
        # ã€Œ## ç·åˆè©•ä¾¡ã€ã€œ æ¬¡è¦‹å‡ºã—ç›´å‰ã¾ã§ã‚’æŠ½å‡ºã—ã€Noto Sans JPã§è¡¨ç¤º
        summary_block = None
        try:
            pattern = r"(?:^|\n)##\s*ç·åˆè©•ä¾¡[\s\S]*?(?=\n##\s|\Z)"
            m = re.search(pattern, report_md)
            if m:
                summary_block = m.group(0)
        except Exception:
            summary_block = None

        st.markdown("#### 5) è§£æçµæœ")
        if summary_block:
            st.markdown('<div class="md-card good-shadow jp-report">', unsafe_allow_html=True)
            st.markdown('<div class="md-title">ğŸ§­ ç·åˆè©•ä¾¡ï¼ˆè¦ç´„ï¼‰</div>', unsafe_allow_html=True)
            st.markdown(f"<div class='jp-report'>{summary_block}</div>", unsafe_allow_html=True)
            st.markdown('</div>', unsafe_allow_html=True)

        # å…¨æ–‡ï¼ˆè©³ç´°ï¼‰ã¯Expanderå†…ã‚’æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã§
        with st.expander("è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤º", expanded=(summary_block is None)):
            st.markdown(f"<div class='jp-report'>{report_md}</div>", unsafe_allow_html=True)

        # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå…±æœ‰ï¼‰â€” Markdownã¯ãƒ•ã‚©ãƒ³ãƒˆã‚’åŸ‹ã‚è¾¼ã¾ãªã„ç‚¹ã«æ³¨æ„
        st.download_button(
            "ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
            report_md,
            file_name="building_health_report.md",
            mime="text/markdown",
            use_container_width=True
        )

        # å‚è€ƒï¼šä½¿ç”¨ã—ãŸRAGæŠœç²‹ï¼ˆã“ã¡ã‚‰ã‚‚æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã§ï¼‰
        with st.expander("ï¼ˆå‚è€ƒï¼‰ä½¿ç”¨ã—ãŸRAGæŠœç²‹"):
            for src, ch in snippets:
                snippet = ch[:600] + ('...' if len(ch) > 600 else '')
                st.markdown(f"<div class='jp-report'><b>{src}</b>ï¼š{snippet}</div>", unsafe_allow_html=True)

    # ãƒ•ãƒƒã‚¿
    st.markdown("")
    st.caption("Â© å»ºç‰©è¨ºæ–­ãã‚“ â€” å¯è¦–/èµ¤å¤– Ã— RAG Ã— Geminiã€‚ãƒ¢ãƒã‚¤ãƒ«æœ€é©åŒ–UIã€‚")


if __name__ == "__main__":
    main()
